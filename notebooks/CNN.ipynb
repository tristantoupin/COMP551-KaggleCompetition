{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of CNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "sDGhFDtdP_0K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.utils import class_weight\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import random\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from scipy import stats\n",
        "import math\n",
        "from skimage import morphology, img_as_ubyte\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nt0OtpF6WXXa",
        "colab_type": "code",
        "outputId": "676befbe-5bdc-4e73-a8c3-f5822921be36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MuK3fO6SP_0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_to_data = \"gdrive/My Drive/data/train_images.npy\"\n",
        "path_to_labels = \"gdrive/My Drive/data/train_labels.csv\"\n",
        "path_to_test = \"gdrive/My Drive/data/test_images.npy\"\n",
        "\n",
        "# path_broad = \"gdrive/My Drive/Mcgill/U4/Fall 2018/COMP 551/kaggle/data/\"\n",
        "# path_to_data = path_broad + \"train_images.npy\"\n",
        "# path_to_labels = path_broad + \"train_labels.csv\"\n",
        "# path_to_test = path_broad + \"test_images.npy\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uA-4nzfSP_0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_images(list_of_images, max_col = 4):\n",
        "    n = len(list_of_images)\n",
        "    if n == 1:\n",
        "        plt.imshow(list_of_images[0], cmap=\"gray\"); plt.axis('off'); plt.show()\n",
        "    else:\n",
        "        # get number of columns and rows required\n",
        "        r, c = 1, n\n",
        "        if n > max_col:\n",
        "            c = max_col\n",
        "            r = int(math.ceil(n/max_col))\n",
        "    \n",
        "        fig = plt.figure(figsize=(20, max_col * r))\n",
        "        for i, (img,name) in enumerate(list_of_images):\n",
        "            ax = fig.add_subplot(r, c, (i+1))\n",
        "            ax.set_title(str(name))\n",
        "            ax.axis('off')\n",
        "            ax.imshow(img, cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZRn5lD6P_0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(path_x, path_y):\n",
        "    data = np.load(path_x, encoding = 'bytes')\n",
        "    labels_df = pd.read_csv(path_y)\n",
        "    labels_df.Category = pd.Categorical(labels_df.Category)\n",
        "    y = labels_df.Category.cat.codes.values\n",
        "    X = np.array(data[:, 1])\n",
        "    X_actual = np.array(data[:, 1])\n",
        "    for c, i in enumerate(X):\n",
        "        ret,thresh_img = cv2.threshold(i,127,255,cv2.THRESH_BINARY)\n",
        "        X[c] = thresh_img\n",
        "    return X, y, X_actual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VqFde78xP_0d",
        "colab_type": "code",
        "outputId": "8b5b7030-500b-4619-951d-c7fcce16c63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "X_noisy, y, X_noisy_actual = get_data(path_to_data, path_to_labels)\n",
        "print(\"Shape of training dataset:\", len(X_noisy))\n",
        "X_test = list(np.load(path_to_test, encoding = 'bytes')[:, 1])\n",
        "print(\"Shape of testing dataset:\", len(X_test))\n",
        "unique_classes = set(y)\n",
        "print(\"Number of classes:\", len(unique_classes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training dataset: 10000\n",
            "Shape of testing dataset: 10000\n",
            "Number of classes: 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DgbenH5VqOnE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RXrUpdFhP_0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def remove_noise(filledImg, min_size = 500):\n",
        "    min_blobs, min_val = ndimage.label(filledImg)\n",
        "    \n",
        "    for i in range(10,60):\n",
        "        clean_img = morphology.remove_small_objects(img_as_bool(filledImg), i)\n",
        "        blobs, number_of_blobs = ndimage.label(clean_img)\n",
        "        if number_of_blobs < min_val:\n",
        "            min_val = number_of_blobs\n",
        "            if min_val == 1:\n",
        "                return clean_img\n",
        "  \n",
        "    return clean_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TqY99QqOP_1a",
        "colab_type": "code",
        "outputId": "56c40faf-d501-4d35-a319-b41c6840d420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "X = []\n",
        "for c, (img, label) in enumerate(zip(X_noisy, y)):\n",
        "    img = img.reshape(100, 100)\n",
        "    X.append(remove_noise(img, 100).flatten()) \n",
        "    \n",
        "test= X[0]\n",
        "test.resize(100,100)\n",
        "plot_images(np.array([test]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:118: UserWarning: Possible sign loss when converting negative image of type float64 to positive image of type bool.\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to bool\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFMCAYAAABCsp4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABOJJREFUeJzt3LFuwjAUQNG6yv//sjtUqliCbgV2\nAJ0zMcWZrt5TEsacc34BcNf31TcA8A7EEiAQS4BALAECsQQIxBIgEEuAQCwBgmPHIWOMHccAPOTe\nNzomS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuA\nQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKx\nBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIg\nEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAs\nAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQI\nxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIDiu\nvoFPN+e85NwxxiXnwqcyWQIEYgkQWMMX++86fLa2W6vhWiZLgEAsAQKxBAjEEiAQS4BALAECsQQI\nxBIgEEuAQCwBArEECMQSIBBLgEAsAQJ/0fbC/C0bvA6TJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJ\nEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAg\nlgCBWAIEYgkQiCVAIJYAgVgCBMfVN8Aac86/32OMC+8EPoPJEiAQS4DAGr7A7Qq88zrWbVjHZAkQ\niCVAYA3f6HZNftaqfmvFNYFfJkuAQCwBAmv4YmdPqFc8ubaGwzomS4BALAECa/hiq7/RPlu9vaAO\nz2WyBAjEEiCwhi9w9vL5Iyt5edJt9YZ1TJYAgVgCBNbwxcpK/sg1gT1MlgCBWAIE1vCNrM/wvkyW\nAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIE\nYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIgl\nQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCB\nWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJ\nEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAg\nlgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgC\nBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgTHjkPm\nnDuOAVjGZAkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgC\nBGIJEIglQCCWAIFYAgRiCRCIJUAglgDBD+BVObHQvzJwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7faffce9b320>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "0eXGh_j9P_1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for c, img in enumerate(X):\n",
        "    X[c] = np.array(img)\n",
        "X = np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WcisegQtcDf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def crop_center(img,cropx,cropy):\n",
        "    y,x = img.shape\n",
        "    startx = x//2-(cropx//2)\n",
        "    starty = y//2-(cropy//2)    \n",
        "    return img[starty:starty+cropy,startx:startx+cropx]\n",
        "  \n",
        "def pad_image(img, pad_t, pad_r, pad_b, pad_l):\n",
        "    \"\"\"Add padding of zeroes to an image.\n",
        "    Add padding to an array image.\n",
        "    :param img:\n",
        "    :param pad_t:\n",
        "    :param pad_r:\n",
        "    :param pad_b:\n",
        "    :param pad_l:\n",
        "    \"\"\"\n",
        "    height, width = img.shape\n",
        "\n",
        "    # Adding padding to the left side.\n",
        "    pad_left = np.zeros((height, pad_l), dtype = np.int)\n",
        "    img = np.concatenate((pad_left, img), axis = 1)\n",
        "\n",
        "    # Adding padding to the top.\n",
        "    pad_up = np.zeros((pad_t, pad_l + width))\n",
        "    img = np.concatenate((pad_up, img), axis = 0)\n",
        "\n",
        "    # Adding padding to the right.\n",
        "    pad_right = np.zeros((height + pad_t, pad_r))\n",
        "    img = np.concatenate((img, pad_right), axis = 1)\n",
        "\n",
        "    # Adding padding to the bottom\n",
        "    pad_bottom = np.zeros((pad_b, pad_l + width + pad_r))\n",
        "    img = np.concatenate((img, pad_bottom), axis = 0)\n",
        "\n",
        "    return img\n",
        "\n",
        "def center_image(X_array, X_noise, show_image=False):\n",
        "  \"\"\"Return a centered image.\n",
        "  :param img:\n",
        "  \"\"\"\n",
        "  cropped = []\n",
        "  for i in range(len(X_array)):\n",
        "    img = X_array[i].copy()\n",
        "    img.resize(100,100)\n",
        "    \n",
        "    img_noise = X_noise[i].copy()\n",
        "    img_noise.resize(100,100)\n",
        "\n",
        "    blobs, min_val = ndimage.label(img)\n",
        "    unique, counts = np.unique(blobs, return_counts=True)\n",
        "    if len(unique) > 1:\n",
        "      id = np.argpartition(counts.flatten(), -2)[-2]\n",
        "      img[blobs != id] = 0    \n",
        "       \n",
        "        \n",
        "    if show_image:\n",
        "      print('actual')\n",
        "      plot_images(np.array([img_noise]))\n",
        "      print('denoised')\n",
        "      plot_images(np.array([img]))\n",
        "      \n",
        "#     cropBox = (min(non_empty_rows), max(non_empty_rows), min(non_empty_columns), max(non_empty_columns))\n",
        "#     y1, y2 = cropBox[0], cropBox[1]\n",
        "#     x1, x2 = cropBox[2], cropBox[3]\n",
        "#     cropped_image = img[y1:y2, x1:x2]\n",
        "\n",
        "    col_sum = np.where(np.sum(img, axis=0) > 0)\n",
        "    row_sum = np.where(np.sum(img, axis=1) > 0)\n",
        "    if len(row_sum[0]) == 0:\n",
        "      y1, y2 = 0, 0\n",
        " \n",
        "    else:\n",
        "      y1, y2 = row_sum[0][0]-5, row_sum[0][-1]+5\n",
        "      if y1 < 0:\n",
        "        y1 = 0\n",
        "      if y2 > img.shape[0]:\n",
        "        y2 = img.shape[0]-1\n",
        "    \n",
        "    if len(col_sum[0]) == 0:\n",
        "      x1, x2 = 0, 0 \n",
        "    else:\n",
        "      x1, x2 = col_sum[0][0]-5, col_sum[0][-1]+5\n",
        "      if x1 < 0:\n",
        "        x1 = 0\n",
        "      if x2 >= img.shape[1]:\n",
        "        x2 = img.shape[1]-1\n",
        "\n",
        "    cropped_image = img_noise[y1:y2, x1:x2]\n",
        "    cropped_denoised = img[y1:y2, x1:x2]\n",
        "    zero_axis_fill = (img.shape[0] - cropped_image.shape[0])\n",
        "    one_axis_fill = (img.shape[1] - cropped_image.shape[1])\n",
        "\n",
        "    top = zero_axis_fill // 2\n",
        "    bottom = zero_axis_fill - top\n",
        "    left = one_axis_fill // 2\n",
        "    right = one_axis_fill - left\n",
        "\n",
        "    padded_image = pad_image(cropped_image, top, left, bottom, right)\n",
        "    padded_denoised = pad_image(cropped_denoised, top, left, bottom, right)\n",
        "    cropped_image = crop_center(padded_image, 40, 40)\n",
        "    cropped_denoised = crop_center(padded_denoised, 40, 40)\n",
        "    if show_image:\n",
        "      print('cropped actual')\n",
        "      plot_images(np.array([cropped_image]))\n",
        "      print('cropped denoised')\n",
        "      plot_images(np.array([cropped_denoised]))\n",
        "\n",
        "    cropped.append(np.array(cropped_image.flatten()))\n",
        "    \n",
        "    \n",
        "\n",
        "  cropped = np.array(cropped)\n",
        "  \n",
        "  return cropped\n",
        "\n",
        "X_cropped = center_image(X, X_noisy_actual, show_image=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8G62U9lP_11",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X_cropped = crop_image(X, size_of_crop=32)\n",
        "unique, counts = np.unique(y[:9488], return_counts=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4xkhRKcku6J",
        "colab_type": "code",
        "outputId": "d14dc725-0e24-46ed-cf83-60ccac3bf476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2958
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 31\n",
        "epochs = 40\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 40, 40\n",
        "# the data, split between train and val sets\n",
        "(x_train, y_train), (x_val, y_val) = (X_cropped[:9488],y[:9488]), (X_cropped[9488:],y[9488:]) \n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "\n",
        "\n",
        "x_train /= 255\n",
        "x_val /= 255\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_val.shape[0], 'val samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "print(y_train[3])\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "datagen.fit(x_train)\n",
        "val_datagen = ImageDataGenerator()\n",
        "val_datagen.fit(x_val)\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "validation_generator = val_datagen.flow(x_val, y_val, batch_size=batch_size)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu',input_shape=input_shape))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# optimizer = keras.optimizers.Nadam(lr=0.002,\n",
        "#                   beta_1=0.9,\n",
        "#                   beta_2=0.999,\n",
        "#                   epsilon=1e-08,\n",
        "#                   schedule_decay=0.004)\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=0.002,\n",
        "                  beta_1=0.9,\n",
        "                  beta_2=0.999,\n",
        "                  epsilon=None,\n",
        "                  schedule_decay=0.004)\n",
        "\n",
        "# Set our optimizer and loss function (similar settings to our CAE approach)\n",
        "model.compile(loss = keras.losses.categorical_crossentropy,\n",
        "            optimizer = optimizer,\n",
        "            metrics = ['categorical_accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('best.hdf5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_categorical_accuracy', \n",
        "                      factor=0.1, patience=5,\n",
        "                      min_delta=0.0001, mode='auto',\n",
        "                      cooldown=0, verbose=1, \n",
        "                      min_lr=0),\n",
        "    checkpoint\n",
        "]\n",
        "model.fit_generator(train_generator,\n",
        "          steps_per_epoch= 14000 // batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1, \n",
        "          validation_data=(x_val, y_val),\n",
        "          callbacks = callbacks,\n",
        "          )\n",
        "\n",
        "\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (9488, 40, 40, 1)\n",
            "9488 train samples\n",
            "512 val samples\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 1/40\n",
            "218/218 [==============================] - 19s 85ms/step - loss: 3.3235 - categorical_accuracy: 0.0717 - val_loss: 3.0211 - val_categorical_accuracy: 0.0977\n",
            "\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.09766, saving model to best.hdf5\n",
            "Epoch 2/40\n",
            "218/218 [==============================] - 10s 44ms/step - loss: 2.7286 - categorical_accuracy: 0.2065 - val_loss: 2.1108 - val_categorical_accuracy: 0.3594\n",
            "\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.09766 to 0.35938, saving model to best.hdf5\n",
            "Epoch 3/40\n",
            "218/218 [==============================] - 10s 44ms/step - loss: 2.2773 - categorical_accuracy: 0.3291 - val_loss: 1.7015 - val_categorical_accuracy: 0.5039\n",
            "\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.35938 to 0.50391, saving model to best.hdf5\n",
            "Epoch 4/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 2.0212 - categorical_accuracy: 0.4019 - val_loss: 1.5189 - val_categorical_accuracy: 0.5488\n",
            "\n",
            "Epoch 00004: val_categorical_accuracy improved from 0.50391 to 0.54883, saving model to best.hdf5\n",
            "Epoch 5/40\n",
            "218/218 [==============================] - 10s 44ms/step - loss: 1.8631 - categorical_accuracy: 0.4554 - val_loss: 1.3630 - val_categorical_accuracy: 0.5938\n",
            "\n",
            "Epoch 00005: val_categorical_accuracy improved from 0.54883 to 0.59375, saving model to best.hdf5\n",
            "Epoch 6/40\n",
            "218/218 [==============================] - 10s 44ms/step - loss: 1.7327 - categorical_accuracy: 0.5032 - val_loss: 1.3092 - val_categorical_accuracy: 0.6328\n",
            "\n",
            "Epoch 00006: val_categorical_accuracy improved from 0.59375 to 0.63281, saving model to best.hdf5\n",
            "Epoch 7/40\n",
            "218/218 [==============================] - 9s 44ms/step - loss: 1.6378 - categorical_accuracy: 0.5288 - val_loss: 1.1369 - val_categorical_accuracy: 0.6836\n",
            "\n",
            "Epoch 00007: val_categorical_accuracy improved from 0.63281 to 0.68359, saving model to best.hdf5\n",
            "Epoch 8/40\n",
            "218/218 [==============================] - 10s 44ms/step - loss: 1.5618 - categorical_accuracy: 0.5531 - val_loss: 1.0967 - val_categorical_accuracy: 0.6875\n",
            "\n",
            "Epoch 00008: val_categorical_accuracy improved from 0.68359 to 0.68750, saving model to best.hdf5\n",
            "Epoch 9/40\n",
            "218/218 [==============================] - 10s 45ms/step - loss: 1.4948 - categorical_accuracy: 0.5717 - val_loss: 1.0785 - val_categorical_accuracy: 0.7109\n",
            "\n",
            "Epoch 00009: val_categorical_accuracy improved from 0.68750 to 0.71094, saving model to best.hdf5\n",
            "Epoch 10/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.4430 - categorical_accuracy: 0.5874 - val_loss: 1.0864 - val_categorical_accuracy: 0.7090\n",
            "\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.71094\n",
            "Epoch 11/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.4052 - categorical_accuracy: 0.6002 - val_loss: 1.0446 - val_categorical_accuracy: 0.7129\n",
            "\n",
            "Epoch 00011: val_categorical_accuracy improved from 0.71094 to 0.71289, saving model to best.hdf5\n",
            "Epoch 12/40\n",
            "218/218 [==============================] - 9s 44ms/step - loss: 1.3653 - categorical_accuracy: 0.6054 - val_loss: 0.9869 - val_categorical_accuracy: 0.7070\n",
            "\n",
            "Epoch 00012: val_categorical_accuracy did not improve from 0.71289\n",
            "Epoch 13/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.3293 - categorical_accuracy: 0.6223 - val_loss: 1.0615 - val_categorical_accuracy: 0.7148\n",
            "\n",
            "Epoch 00013: val_categorical_accuracy improved from 0.71289 to 0.71484, saving model to best.hdf5\n",
            "Epoch 14/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.3246 - categorical_accuracy: 0.6245 - val_loss: 0.9428 - val_categorical_accuracy: 0.7422\n",
            "\n",
            "Epoch 00014: val_categorical_accuracy improved from 0.71484 to 0.74219, saving model to best.hdf5\n",
            "Epoch 15/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.2884 - categorical_accuracy: 0.6350 - val_loss: 0.9111 - val_categorical_accuracy: 0.7266\n",
            "\n",
            "Epoch 00015: val_categorical_accuracy did not improve from 0.74219\n",
            "Epoch 16/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.2610 - categorical_accuracy: 0.6398 - val_loss: 0.8816 - val_categorical_accuracy: 0.7383\n",
            "\n",
            "Epoch 00016: val_categorical_accuracy did not improve from 0.74219\n",
            "Epoch 17/40\n",
            "218/218 [==============================] - 10s 46ms/step - loss: 1.2400 - categorical_accuracy: 0.6501 - val_loss: 0.9072 - val_categorical_accuracy: 0.7422\n",
            "\n",
            "Epoch 00017: val_categorical_accuracy did not improve from 0.74219\n",
            "Epoch 18/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.2273 - categorical_accuracy: 0.6502 - val_loss: 0.8994 - val_categorical_accuracy: 0.7578\n",
            "\n",
            "Epoch 00018: val_categorical_accuracy improved from 0.74219 to 0.75781, saving model to best.hdf5\n",
            "Epoch 19/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.2191 - categorical_accuracy: 0.6504 - val_loss: 0.8523 - val_categorical_accuracy: 0.7676\n",
            "\n",
            "Epoch 00019: val_categorical_accuracy improved from 0.75781 to 0.76758, saving model to best.hdf5\n",
            "Epoch 20/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.2172 - categorical_accuracy: 0.6475 - val_loss: 0.8282 - val_categorical_accuracy: 0.7578\n",
            "\n",
            "Epoch 00020: val_categorical_accuracy did not improve from 0.76758\n",
            "Epoch 21/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.1812 - categorical_accuracy: 0.6608 - val_loss: 0.8660 - val_categorical_accuracy: 0.7617\n",
            "\n",
            "Epoch 00021: val_categorical_accuracy did not improve from 0.76758\n",
            "Epoch 22/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.1611 - categorical_accuracy: 0.6690 - val_loss: 0.8720 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00022: val_categorical_accuracy did not improve from 0.76758\n",
            "Epoch 23/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.1588 - categorical_accuracy: 0.6735 - val_loss: 0.8421 - val_categorical_accuracy: 0.7812\n",
            "\n",
            "Epoch 00023: val_categorical_accuracy improved from 0.76758 to 0.78125, saving model to best.hdf5\n",
            "Epoch 24/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.1655 - categorical_accuracy: 0.6657 - val_loss: 0.8525 - val_categorical_accuracy: 0.7676\n",
            "\n",
            "Epoch 00024: val_categorical_accuracy did not improve from 0.78125\n",
            "Epoch 25/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.1323 - categorical_accuracy: 0.6757 - val_loss: 0.8469 - val_categorical_accuracy: 0.7773\n",
            "\n",
            "Epoch 00025: val_categorical_accuracy did not improve from 0.78125\n",
            "Epoch 26/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.1531 - categorical_accuracy: 0.6729 - val_loss: 0.8250 - val_categorical_accuracy: 0.7676\n",
            "\n",
            "Epoch 00026: val_categorical_accuracy did not improve from 0.78125\n",
            "Epoch 27/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.1179 - categorical_accuracy: 0.6839 - val_loss: 0.8347 - val_categorical_accuracy: 0.7676\n",
            "\n",
            "Epoch 00027: val_categorical_accuracy did not improve from 0.78125\n",
            "Epoch 28/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 1.1114 - categorical_accuracy: 0.6821 - val_loss: 0.8514 - val_categorical_accuracy: 0.7695\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\n",
            "Epoch 00028: val_categorical_accuracy did not improve from 0.78125\n",
            "Epoch 29/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.0362 - categorical_accuracy: 0.7063 - val_loss: 0.7776 - val_categorical_accuracy: 0.7910\n",
            "\n",
            "Epoch 00029: val_categorical_accuracy improved from 0.78125 to 0.79102, saving model to best.hdf5\n",
            "Epoch 30/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 1.0270 - categorical_accuracy: 0.7073 - val_loss: 0.7985 - val_categorical_accuracy: 0.7891\n",
            "\n",
            "Epoch 00030: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 31/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9968 - categorical_accuracy: 0.7117 - val_loss: 0.7870 - val_categorical_accuracy: 0.7871\n",
            "\n",
            "Epoch 00031: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 32/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9966 - categorical_accuracy: 0.7147 - val_loss: 0.7810 - val_categorical_accuracy: 0.7910\n",
            "\n",
            "Epoch 00032: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 33/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9976 - categorical_accuracy: 0.7122 - val_loss: 0.7727 - val_categorical_accuracy: 0.7832\n",
            "\n",
            "Epoch 00033: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 34/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 0.9692 - categorical_accuracy: 0.7215 - val_loss: 0.7773 - val_categorical_accuracy: 0.7832\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
            "\n",
            "Epoch 00034: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 35/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9748 - categorical_accuracy: 0.7181 - val_loss: 0.7734 - val_categorical_accuracy: 0.7832\n",
            "\n",
            "Epoch 00035: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 36/40\n",
            "218/218 [==============================] - 9s 43ms/step - loss: 0.9616 - categorical_accuracy: 0.7231 - val_loss: 0.7704 - val_categorical_accuracy: 0.7832\n",
            "\n",
            "Epoch 00036: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 37/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9740 - categorical_accuracy: 0.7195 - val_loss: 0.7724 - val_categorical_accuracy: 0.7793\n",
            "\n",
            "Epoch 00037: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 38/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9590 - categorical_accuracy: 0.7244 - val_loss: 0.7711 - val_categorical_accuracy: 0.7832\n",
            "\n",
            "Epoch 00038: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 39/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9804 - categorical_accuracy: 0.7176 - val_loss: 0.7698 - val_categorical_accuracy: 0.7812\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
            "\n",
            "Epoch 00039: val_categorical_accuracy did not improve from 0.79102\n",
            "Epoch 40/40\n",
            "218/218 [==============================] - 9s 42ms/step - loss: 0.9645 - categorical_accuracy: 0.7218 - val_loss: 0.7693 - val_categorical_accuracy: 0.7812\n",
            "\n",
            "Epoch 00040: val_categorical_accuracy did not improve from 0.79102\n",
            "Test loss: 0.7692752331495285\n",
            "Test accuracy: 0.78125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ldA0n91xrQ7G",
        "colab_type": "code",
        "outputId": "f5402dfb-a27c-48e3-a964-d778fe4e2967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3862
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "\n",
        "def top_3_accuracy(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "  \n",
        "  \n",
        "batch_size = 64\n",
        "num_classes = 31\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 40, 40\n",
        "# the data, split between train and val sets\n",
        "(x_train, y_train), (x_val, y_val) = (X_cropped[:9488],y[:9488]), (X_cropped[9488:],y[9488:]) \n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_val /= 244\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_val.shape[0], 'val samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "print(y_train[3])\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=0,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "datagen.fit(x_train)\n",
        "val_datagen = ImageDataGenerator()\n",
        "val_datagen.fit(x_val)\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "validation_generator = val_datagen.flow(x_val, y_val, batch_size=batch_size)\n",
        "\n",
        "model_mn = MobileNet(input_shape=(img_rows, img_cols, 1), alpha=1., weights=None, classes=num_classes)\n",
        "model_mn.compile(optimizer=Adam(lr=0.002), loss='categorical_crossentropy',\n",
        "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\n",
        "\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5,\n",
        "                      min_delta=0.005, mode='max', cooldown=3, verbose=1)\n",
        "]\n",
        "\n",
        "hist = model_mn.fit_generator(\n",
        "    train_generator, steps_per_epoch= 14000 // batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks = callbacks\n",
        ")\n",
        "\n",
        "score = model_mn.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (9488, 40, 40, 1)\n",
            "9488 train samples\n",
            "512 val samples\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 1/100\n",
            "218/218 [==============================] - 27s 122ms/step - loss: 3.2501 - categorical_crossentropy: 3.2501 - categorical_accuracy: 0.1277 - top_3_accuracy: 0.2977 - val_loss: 3.4903 - val_categorical_crossentropy: 3.4903 - val_categorical_accuracy: 0.0293 - val_top_3_accuracy: 0.1191\n",
            "Epoch 2/100\n",
            "218/218 [==============================] - 18s 81ms/step - loss: 2.8529 - categorical_crossentropy: 2.8529 - categorical_accuracy: 0.1927 - top_3_accuracy: 0.4261 - val_loss: 3.5729 - val_categorical_crossentropy: 3.5729 - val_categorical_accuracy: 0.0488 - val_top_3_accuracy: 0.1328\n",
            "Epoch 3/100\n",
            "218/218 [==============================] - 18s 81ms/step - loss: 2.6641 - categorical_crossentropy: 2.6641 - categorical_accuracy: 0.2436 - top_3_accuracy: 0.5031 - val_loss: 3.7969 - val_categorical_crossentropy: 3.7969 - val_categorical_accuracy: 0.0801 - val_top_3_accuracy: 0.2070\n",
            "Epoch 4/100\n",
            "218/218 [==============================] - 18s 82ms/step - loss: 2.8399 - categorical_crossentropy: 2.8399 - categorical_accuracy: 0.2284 - top_3_accuracy: 0.4778 - val_loss: 4.5307 - val_categorical_crossentropy: 4.5307 - val_categorical_accuracy: 0.1309 - val_top_3_accuracy: 0.2637\n",
            "Epoch 5/100\n",
            "218/218 [==============================] - 18s 81ms/step - loss: 2.6982 - categorical_crossentropy: 2.6982 - categorical_accuracy: 0.2542 - top_3_accuracy: 0.5159 - val_loss: 3.5411 - val_categorical_crossentropy: 3.5411 - val_categorical_accuracy: 0.1758 - val_top_3_accuracy: 0.3789\n",
            "Epoch 6/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 2.5293 - categorical_crossentropy: 2.5293 - categorical_accuracy: 0.2956 - top_3_accuracy: 0.5573 - val_loss: 5.9121 - val_categorical_crossentropy: 5.9121 - val_categorical_accuracy: 0.1934 - val_top_3_accuracy: 0.3926\n",
            "Epoch 7/100\n",
            "218/218 [==============================] - 18s 82ms/step - loss: 2.4310 - categorical_crossentropy: 2.4310 - categorical_accuracy: 0.3159 - top_3_accuracy: 0.5786 - val_loss: 2.8961 - val_categorical_crossentropy: 2.8961 - val_categorical_accuracy: 0.1953 - val_top_3_accuracy: 0.4531\n",
            "Epoch 8/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 2.2592 - categorical_crossentropy: 2.2592 - categorical_accuracy: 0.3663 - top_3_accuracy: 0.6348 - val_loss: 2.9558 - val_categorical_crossentropy: 2.9558 - val_categorical_accuracy: 0.2422 - val_top_3_accuracy: 0.4453\n",
            "Epoch 9/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 2.3054 - categorical_crossentropy: 2.3054 - categorical_accuracy: 0.3598 - top_3_accuracy: 0.6324 - val_loss: 2.7668 - val_categorical_crossentropy: 2.7668 - val_categorical_accuracy: 0.2539 - val_top_3_accuracy: 0.4629\n",
            "Epoch 10/100\n",
            "218/218 [==============================] - 18s 80ms/step - loss: 2.0039 - categorical_crossentropy: 2.0039 - categorical_accuracy: 0.4070 - top_3_accuracy: 0.6729 - val_loss: 2.0753 - val_categorical_crossentropy: 2.0753 - val_categorical_accuracy: 0.3945 - val_top_3_accuracy: 0.6602\n",
            "Epoch 11/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 1.8769 - categorical_crossentropy: 1.8769 - categorical_accuracy: 0.4361 - top_3_accuracy: 0.7080 - val_loss: 2.2628 - val_categorical_crossentropy: 2.2628 - val_categorical_accuracy: 0.3828 - val_top_3_accuracy: 0.6406\n",
            "Epoch 12/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.8213 - categorical_crossentropy: 1.8213 - categorical_accuracy: 0.4535 - top_3_accuracy: 0.7181 - val_loss: 2.2385 - val_categorical_crossentropy: 2.2385 - val_categorical_accuracy: 0.3652 - val_top_3_accuracy: 0.6523\n",
            "Epoch 13/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.7586 - categorical_crossentropy: 1.7586 - categorical_accuracy: 0.4789 - top_3_accuracy: 0.7315 - val_loss: 1.7809 - val_categorical_crossentropy: 1.7809 - val_categorical_accuracy: 0.4941 - val_top_3_accuracy: 0.7383\n",
            "Epoch 14/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.6969 - categorical_crossentropy: 1.6969 - categorical_accuracy: 0.4924 - top_3_accuracy: 0.7456 - val_loss: 2.2277 - val_categorical_crossentropy: 2.2277 - val_categorical_accuracy: 0.4160 - val_top_3_accuracy: 0.6875\n",
            "Epoch 15/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.7565 - categorical_crossentropy: 1.7565 - categorical_accuracy: 0.4783 - top_3_accuracy: 0.7365 - val_loss: 2.2924 - val_categorical_crossentropy: 2.2924 - val_categorical_accuracy: 0.3613 - val_top_3_accuracy: 0.6094\n",
            "Epoch 16/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.6594 - categorical_crossentropy: 1.6594 - categorical_accuracy: 0.5071 - top_3_accuracy: 0.7549 - val_loss: 1.8560 - val_categorical_crossentropy: 1.8560 - val_categorical_accuracy: 0.4727 - val_top_3_accuracy: 0.7285\n",
            "Epoch 17/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.6182 - categorical_crossentropy: 1.6182 - categorical_accuracy: 0.5212 - top_3_accuracy: 0.7675 - val_loss: 1.6582 - val_categorical_crossentropy: 1.6582 - val_categorical_accuracy: 0.5293 - val_top_3_accuracy: 0.7773\n",
            "Epoch 18/100\n",
            "218/218 [==============================] - 18s 81ms/step - loss: 1.5360 - categorical_crossentropy: 1.5360 - categorical_accuracy: 0.5490 - top_3_accuracy: 0.7863 - val_loss: 1.8321 - val_categorical_crossentropy: 1.8321 - val_categorical_accuracy: 0.5098 - val_top_3_accuracy: 0.7441\n",
            "Epoch 19/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 1.5190 - categorical_crossentropy: 1.5190 - categorical_accuracy: 0.5533 - top_3_accuracy: 0.7873 - val_loss: 1.8331 - val_categorical_crossentropy: 1.8331 - val_categorical_accuracy: 0.4883 - val_top_3_accuracy: 0.7520\n",
            "Epoch 20/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.4478 - categorical_crossentropy: 1.4478 - categorical_accuracy: 0.5730 - top_3_accuracy: 0.8023 - val_loss: 1.7157 - val_categorical_crossentropy: 1.7157 - val_categorical_accuracy: 0.5273 - val_top_3_accuracy: 0.7461\n",
            "Epoch 21/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.4552 - categorical_crossentropy: 1.4552 - categorical_accuracy: 0.5761 - top_3_accuracy: 0.7995 - val_loss: 1.6453 - val_categorical_crossentropy: 1.6453 - val_categorical_accuracy: 0.5352 - val_top_3_accuracy: 0.7578\n",
            "Epoch 22/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.3919 - categorical_crossentropy: 1.3919 - categorical_accuracy: 0.5963 - top_3_accuracy: 0.8111 - val_loss: 2.1422 - val_categorical_crossentropy: 2.1422 - val_categorical_accuracy: 0.4082 - val_top_3_accuracy: 0.6426\n",
            "Epoch 23/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.3823 - categorical_crossentropy: 1.3823 - categorical_accuracy: 0.5966 - top_3_accuracy: 0.8126 - val_loss: 1.5662 - val_categorical_crossentropy: 1.5662 - val_categorical_accuracy: 0.5742 - val_top_3_accuracy: 0.7578\n",
            "Epoch 24/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.3243 - categorical_crossentropy: 1.3243 - categorical_accuracy: 0.6125 - top_3_accuracy: 0.8257 - val_loss: 1.5170 - val_categorical_crossentropy: 1.5170 - val_categorical_accuracy: 0.6016 - val_top_3_accuracy: 0.7930\n",
            "Epoch 25/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.3253 - categorical_crossentropy: 1.3253 - categorical_accuracy: 0.6183 - top_3_accuracy: 0.8247 - val_loss: 1.2737 - val_categorical_crossentropy: 1.2737 - val_categorical_accuracy: 0.6543 - val_top_3_accuracy: 0.8164\n",
            "Epoch 26/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.2827 - categorical_crossentropy: 1.2827 - categorical_accuracy: 0.6294 - top_3_accuracy: 0.8312 - val_loss: 1.4559 - val_categorical_crossentropy: 1.4559 - val_categorical_accuracy: 0.5957 - val_top_3_accuracy: 0.7832\n",
            "Epoch 27/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.2517 - categorical_crossentropy: 1.2517 - categorical_accuracy: 0.6356 - top_3_accuracy: 0.8339 - val_loss: 1.7425 - val_categorical_crossentropy: 1.7425 - val_categorical_accuracy: 0.5254 - val_top_3_accuracy: 0.7285\n",
            "Epoch 28/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 1.2175 - categorical_crossentropy: 1.2175 - categorical_accuracy: 0.6527 - top_3_accuracy: 0.8412 - val_loss: 1.6114 - val_categorical_crossentropy: 1.6114 - val_categorical_accuracy: 0.5703 - val_top_3_accuracy: 0.7520\n",
            "Epoch 29/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 1.2008 - categorical_crossentropy: 1.2008 - categorical_accuracy: 0.6545 - top_3_accuracy: 0.8478 - val_loss: 1.4736 - val_categorical_crossentropy: 1.4736 - val_categorical_accuracy: 0.5957 - val_top_3_accuracy: 0.7949\n",
            "Epoch 30/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 1.1951 - categorical_crossentropy: 1.1951 - categorical_accuracy: 0.6537 - top_3_accuracy: 0.8432\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
            "218/218 [==============================] - 19s 85ms/step - loss: 1.1949 - categorical_crossentropy: 1.1949 - categorical_accuracy: 0.6537 - top_3_accuracy: 0.8433 - val_loss: 1.8074 - val_categorical_crossentropy: 1.8074 - val_categorical_accuracy: 0.5176 - val_top_3_accuracy: 0.7012\n",
            "Epoch 31/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.0853 - categorical_crossentropy: 1.0853 - categorical_accuracy: 0.6811 - top_3_accuracy: 0.8670 - val_loss: 1.1547 - val_categorical_crossentropy: 1.1547 - val_categorical_accuracy: 0.6836 - val_top_3_accuracy: 0.8438\n",
            "Epoch 32/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.0667 - categorical_crossentropy: 1.0667 - categorical_accuracy: 0.6945 - top_3_accuracy: 0.8705 - val_loss: 2.6091 - val_categorical_crossentropy: 2.6091 - val_categorical_accuracy: 0.4316 - val_top_3_accuracy: 0.6992\n",
            "Epoch 33/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.0706 - categorical_crossentropy: 1.0706 - categorical_accuracy: 0.6879 - top_3_accuracy: 0.8665 - val_loss: 1.2177 - val_categorical_crossentropy: 1.2177 - val_categorical_accuracy: 0.6504 - val_top_3_accuracy: 0.8379\n",
            "Epoch 34/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 1.0447 - categorical_crossentropy: 1.0447 - categorical_accuracy: 0.6973 - top_3_accuracy: 0.8741 - val_loss: 1.1151 - val_categorical_crossentropy: 1.1151 - val_categorical_accuracy: 0.7012 - val_top_3_accuracy: 0.8496\n",
            "Epoch 35/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 1.0144 - categorical_crossentropy: 1.0144 - categorical_accuracy: 0.7066 - top_3_accuracy: 0.8747 - val_loss: 1.1558 - val_categorical_crossentropy: 1.1558 - val_categorical_accuracy: 0.6660 - val_top_3_accuracy: 0.8477\n",
            "Epoch 36/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 1.0014 - categorical_crossentropy: 1.0014 - categorical_accuracy: 0.7103 - top_3_accuracy: 0.8784 - val_loss: 1.1734 - val_categorical_crossentropy: 1.1734 - val_categorical_accuracy: 0.6699 - val_top_3_accuracy: 0.8516\n",
            "Epoch 37/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.9602 - categorical_crossentropy: 0.9602 - categorical_accuracy: 0.7220 - top_3_accuracy: 0.8847 - val_loss: 1.0892 - val_categorical_crossentropy: 1.0892 - val_categorical_accuracy: 0.7090 - val_top_3_accuracy: 0.8594\n",
            "Epoch 38/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.9586 - categorical_crossentropy: 0.9586 - categorical_accuracy: 0.7200 - top_3_accuracy: 0.8890 - val_loss: 1.1467 - val_categorical_crossentropy: 1.1467 - val_categorical_accuracy: 0.6934 - val_top_3_accuracy: 0.8418\n",
            "Epoch 39/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 0.9880 - categorical_crossentropy: 0.9880 - categorical_accuracy: 0.7109 - top_3_accuracy: 0.8837 - val_loss: 1.1517 - val_categorical_crossentropy: 1.1517 - val_categorical_accuracy: 0.6797 - val_top_3_accuracy: 0.8457\n",
            "Epoch 40/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.9593 - categorical_crossentropy: 0.9593 - categorical_accuracy: 0.7208 - top_3_accuracy: 0.8868 - val_loss: 1.0861 - val_categorical_crossentropy: 1.0861 - val_categorical_accuracy: 0.7148 - val_top_3_accuracy: 0.8496\n",
            "Epoch 41/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.9227 - categorical_crossentropy: 0.9227 - categorical_accuracy: 0.7303 - top_3_accuracy: 0.8924 - val_loss: 1.1165 - val_categorical_crossentropy: 1.1165 - val_categorical_accuracy: 0.6992 - val_top_3_accuracy: 0.8496\n",
            "Epoch 42/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.8936 - categorical_crossentropy: 0.8936 - categorical_accuracy: 0.7347 - top_3_accuracy: 0.8978 - val_loss: 1.0943 - val_categorical_crossentropy: 1.0943 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8574\n",
            "Epoch 43/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.9030 - categorical_crossentropy: 0.9030 - categorical_accuracy: 0.7350 - top_3_accuracy: 0.8928 - val_loss: 1.1224 - val_categorical_crossentropy: 1.1224 - val_categorical_accuracy: 0.7031 - val_top_3_accuracy: 0.8496\n",
            "Epoch 44/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.9218 - categorical_crossentropy: 0.9218 - categorical_accuracy: 0.7299 - top_3_accuracy: 0.8906 - val_loss: 1.0064 - val_categorical_crossentropy: 1.0064 - val_categorical_accuracy: 0.7285 - val_top_3_accuracy: 0.8691\n",
            "Epoch 45/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.8927 - categorical_crossentropy: 0.8927 - categorical_accuracy: 0.7372 - top_3_accuracy: 0.8964 - val_loss: 1.2415 - val_categorical_crossentropy: 1.2415 - val_categorical_accuracy: 0.6641 - val_top_3_accuracy: 0.8496\n",
            "Epoch 46/100\n",
            "218/218 [==============================] - 18s 81ms/step - loss: 0.8657 - categorical_crossentropy: 0.8657 - categorical_accuracy: 0.7467 - top_3_accuracy: 0.9005 - val_loss: 1.3699 - val_categorical_crossentropy: 1.3699 - val_categorical_accuracy: 0.6348 - val_top_3_accuracy: 0.8105\n",
            "Epoch 47/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.8788 - categorical_crossentropy: 0.8788 - categorical_accuracy: 0.7410 - top_3_accuracy: 0.9015 - val_loss: 1.5955 - val_categorical_crossentropy: 1.5955 - val_categorical_accuracy: 0.5977 - val_top_3_accuracy: 0.7773\n",
            "Epoch 48/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.8455 - categorical_crossentropy: 0.8455 - categorical_accuracy: 0.7484 - top_3_accuracy: 0.9081 - val_loss: 1.1412 - val_categorical_crossentropy: 1.1412 - val_categorical_accuracy: 0.6934 - val_top_3_accuracy: 0.8477\n",
            "Epoch 49/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.8347 - categorical_crossentropy: 0.8347 - categorical_accuracy: 0.7527 - top_3_accuracy: 0.9072\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.8342 - categorical_crossentropy: 0.8342 - categorical_accuracy: 0.7528 - top_3_accuracy: 0.9073 - val_loss: 1.2785 - val_categorical_crossentropy: 1.2785 - val_categorical_accuracy: 0.6758 - val_top_3_accuracy: 0.8242\n",
            "Epoch 50/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.7760 - categorical_crossentropy: 0.7760 - categorical_accuracy: 0.7696 - top_3_accuracy: 0.9162 - val_loss: 0.9317 - val_categorical_crossentropy: 0.9317 - val_categorical_accuracy: 0.7520 - val_top_3_accuracy: 0.8809\n",
            "Epoch 51/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.7563 - categorical_crossentropy: 0.7563 - categorical_accuracy: 0.7717 - top_3_accuracy: 0.9191 - val_loss: 1.0524 - val_categorical_crossentropy: 1.0524 - val_categorical_accuracy: 0.7305 - val_top_3_accuracy: 0.8672\n",
            "Epoch 52/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.7689 - categorical_crossentropy: 0.7689 - categorical_accuracy: 0.7729 - top_3_accuracy: 0.9149 - val_loss: 1.0610 - val_categorical_crossentropy: 1.0610 - val_categorical_accuracy: 0.7246 - val_top_3_accuracy: 0.8652\n",
            "Epoch 53/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.7323 - categorical_crossentropy: 0.7323 - categorical_accuracy: 0.7776 - top_3_accuracy: 0.9220 - val_loss: 0.9108 - val_categorical_crossentropy: 0.9108 - val_categorical_accuracy: 0.7695 - val_top_3_accuracy: 0.8965\n",
            "Epoch 54/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 0.7275 - categorical_crossentropy: 0.7275 - categorical_accuracy: 0.7820 - top_3_accuracy: 0.9202 - val_loss: 1.0331 - val_categorical_crossentropy: 1.0331 - val_categorical_accuracy: 0.7324 - val_top_3_accuracy: 0.8574\n",
            "Epoch 55/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.7073 - categorical_crossentropy: 0.7073 - categorical_accuracy: 0.7878 - top_3_accuracy: 0.9254 - val_loss: 0.9659 - val_categorical_crossentropy: 0.9659 - val_categorical_accuracy: 0.7559 - val_top_3_accuracy: 0.8887\n",
            "Epoch 56/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.7048 - categorical_crossentropy: 0.7048 - categorical_accuracy: 0.7929 - top_3_accuracy: 0.9250 - val_loss: 1.1319 - val_categorical_crossentropy: 1.1319 - val_categorical_accuracy: 0.7129 - val_top_3_accuracy: 0.8613\n",
            "Epoch 57/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.6995 - categorical_crossentropy: 0.6995 - categorical_accuracy: 0.7870 - top_3_accuracy: 0.9293 - val_loss: 1.0010 - val_categorical_crossentropy: 1.0010 - val_categorical_accuracy: 0.7441 - val_top_3_accuracy: 0.8828\n",
            "Epoch 58/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.6797 - categorical_crossentropy: 0.6797 - categorical_accuracy: 0.7915 - top_3_accuracy: 0.9292\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.6803 - categorical_crossentropy: 0.6803 - categorical_accuracy: 0.7915 - top_3_accuracy: 0.9291 - val_loss: 1.1105 - val_categorical_crossentropy: 1.1105 - val_categorical_accuracy: 0.7227 - val_top_3_accuracy: 0.8574\n",
            "Epoch 59/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.6651 - categorical_crossentropy: 0.6651 - categorical_accuracy: 0.7992 - top_3_accuracy: 0.9303 - val_loss: 0.9496 - val_categorical_crossentropy: 0.9496 - val_categorical_accuracy: 0.7559 - val_top_3_accuracy: 0.8887\n",
            "Epoch 60/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.6535 - categorical_crossentropy: 0.6535 - categorical_accuracy: 0.8037 - top_3_accuracy: 0.9330 - val_loss: 0.9422 - val_categorical_crossentropy: 0.9422 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8926\n",
            "Epoch 61/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.6442 - categorical_crossentropy: 0.6442 - categorical_accuracy: 0.8046 - top_3_accuracy: 0.9349 - val_loss: 0.9833 - val_categorical_crossentropy: 0.9833 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8789\n",
            "Epoch 62/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.6278 - categorical_crossentropy: 0.6278 - categorical_accuracy: 0.8069 - top_3_accuracy: 0.9390 - val_loss: 1.0093 - val_categorical_crossentropy: 1.0093 - val_categorical_accuracy: 0.7520 - val_top_3_accuracy: 0.8730\n",
            "Epoch 63/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.6272 - categorical_crossentropy: 0.6272 - categorical_accuracy: 0.8067 - top_3_accuracy: 0.9364 - val_loss: 1.0271 - val_categorical_crossentropy: 1.0271 - val_categorical_accuracy: 0.7402 - val_top_3_accuracy: 0.8633\n",
            "Epoch 64/100\n",
            "218/218 [==============================] - 17s 79ms/step - loss: 0.6103 - categorical_crossentropy: 0.6103 - categorical_accuracy: 0.8101 - top_3_accuracy: 0.9414 - val_loss: 1.0233 - val_categorical_crossentropy: 1.0233 - val_categorical_accuracy: 0.7441 - val_top_3_accuracy: 0.8613\n",
            "Epoch 65/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.6054 - categorical_crossentropy: 0.6054 - categorical_accuracy: 0.8167 - top_3_accuracy: 0.9384\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.6064 - categorical_crossentropy: 0.6064 - categorical_accuracy: 0.8163 - top_3_accuracy: 0.9384 - val_loss: 1.0073 - val_categorical_crossentropy: 1.0073 - val_categorical_accuracy: 0.7500 - val_top_3_accuracy: 0.8828\n",
            "Epoch 66/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.6010 - categorical_crossentropy: 0.6010 - categorical_accuracy: 0.8137 - top_3_accuracy: 0.9402 - val_loss: 0.9485 - val_categorical_crossentropy: 0.9485 - val_categorical_accuracy: 0.7598 - val_top_3_accuracy: 0.8906\n",
            "Epoch 67/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.5976 - categorical_crossentropy: 0.5976 - categorical_accuracy: 0.8199 - top_3_accuracy: 0.9419 - val_loss: 0.9634 - val_categorical_crossentropy: 0.9634 - val_categorical_accuracy: 0.7520 - val_top_3_accuracy: 0.8887\n",
            "Epoch 68/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5906 - categorical_crossentropy: 0.5906 - categorical_accuracy: 0.8195 - top_3_accuracy: 0.9417 - val_loss: 1.0447 - val_categorical_crossentropy: 1.0447 - val_categorical_accuracy: 0.7559 - val_top_3_accuracy: 0.8750\n",
            "Epoch 69/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.5890 - categorical_crossentropy: 0.5890 - categorical_accuracy: 0.8131 - top_3_accuracy: 0.9438 - val_loss: 0.9712 - val_categorical_crossentropy: 0.9712 - val_categorical_accuracy: 0.7559 - val_top_3_accuracy: 0.8809\n",
            "Epoch 70/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5712 - categorical_crossentropy: 0.5712 - categorical_accuracy: 0.8284 - top_3_accuracy: 0.9451 - val_loss: 0.9587 - val_categorical_crossentropy: 0.9587 - val_categorical_accuracy: 0.7715 - val_top_3_accuracy: 0.8926\n",
            "Epoch 71/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5818 - categorical_crossentropy: 0.5818 - categorical_accuracy: 0.8188 - top_3_accuracy: 0.9445 - val_loss: 0.9897 - val_categorical_crossentropy: 0.9897 - val_categorical_accuracy: 0.7598 - val_top_3_accuracy: 0.8926\n",
            "Epoch 72/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.5766 - categorical_crossentropy: 0.5766 - categorical_accuracy: 0.8193 - top_3_accuracy: 0.9436\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 0.5767 - categorical_crossentropy: 0.5767 - categorical_accuracy: 0.8191 - top_3_accuracy: 0.9437 - val_loss: 1.0190 - val_categorical_crossentropy: 1.0190 - val_categorical_accuracy: 0.7559 - val_top_3_accuracy: 0.8730\n",
            "Epoch 73/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5690 - categorical_crossentropy: 0.5690 - categorical_accuracy: 0.8282 - top_3_accuracy: 0.9437 - val_loss: 0.9956 - val_categorical_crossentropy: 0.9956 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8809\n",
            "Epoch 74/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5509 - categorical_crossentropy: 0.5509 - categorical_accuracy: 0.8293 - top_3_accuracy: 0.9490 - val_loss: 0.9820 - val_categorical_crossentropy: 0.9820 - val_categorical_accuracy: 0.7598 - val_top_3_accuracy: 0.8848\n",
            "Epoch 75/100\n",
            "218/218 [==============================] - 16s 76ms/step - loss: 0.5583 - categorical_crossentropy: 0.5583 - categorical_accuracy: 0.8263 - top_3_accuracy: 0.9467 - val_loss: 1.0128 - val_categorical_crossentropy: 1.0128 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8848\n",
            "Epoch 76/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5532 - categorical_crossentropy: 0.5532 - categorical_accuracy: 0.8263 - top_3_accuracy: 0.9458 - val_loss: 0.9888 - val_categorical_crossentropy: 0.9888 - val_categorical_accuracy: 0.7598 - val_top_3_accuracy: 0.8770\n",
            "Epoch 77/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5550 - categorical_crossentropy: 0.5550 - categorical_accuracy: 0.8280 - top_3_accuracy: 0.9485 - val_loss: 0.9802 - val_categorical_crossentropy: 0.9802 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8770\n",
            "Epoch 78/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.5327 - categorical_crossentropy: 0.5327 - categorical_accuracy: 0.8320 - top_3_accuracy: 0.9510 - val_loss: 0.9951 - val_categorical_crossentropy: 0.9951 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8789\n",
            "Epoch 79/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.5431 - categorical_crossentropy: 0.5431 - categorical_accuracy: 0.8304 - top_3_accuracy: 0.9513\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5424 - categorical_crossentropy: 0.5424 - categorical_accuracy: 0.8306 - top_3_accuracy: 0.9513 - val_loss: 0.9903 - val_categorical_crossentropy: 0.9903 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8828\n",
            "Epoch 80/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5393 - categorical_crossentropy: 0.5393 - categorical_accuracy: 0.8366 - top_3_accuracy: 0.9486 - val_loss: 0.9963 - val_categorical_crossentropy: 0.9963 - val_categorical_accuracy: 0.7637 - val_top_3_accuracy: 0.8809\n",
            "Epoch 81/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5314 - categorical_crossentropy: 0.5314 - categorical_accuracy: 0.8355 - top_3_accuracy: 0.9510 - val_loss: 1.0204 - val_categorical_crossentropy: 1.0204 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8711\n",
            "Epoch 82/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.5457 - categorical_crossentropy: 0.5457 - categorical_accuracy: 0.8298 - top_3_accuracy: 0.9483 - val_loss: 1.0143 - val_categorical_crossentropy: 1.0143 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8691\n",
            "Epoch 83/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5281 - categorical_crossentropy: 0.5281 - categorical_accuracy: 0.8346 - top_3_accuracy: 0.9503 - val_loss: 0.9983 - val_categorical_crossentropy: 0.9983 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8789\n",
            "Epoch 84/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5303 - categorical_crossentropy: 0.5303 - categorical_accuracy: 0.8339 - top_3_accuracy: 0.9516 - val_loss: 0.9970 - val_categorical_crossentropy: 0.9970 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8730\n",
            "Epoch 85/100\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5302 - categorical_crossentropy: 0.5302 - categorical_accuracy: 0.8350 - top_3_accuracy: 0.9514 - val_loss: 0.9853 - val_categorical_crossentropy: 0.9853 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8828\n",
            "Epoch 86/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.5439 - categorical_crossentropy: 0.5439 - categorical_accuracy: 0.8310 - top_3_accuracy: 0.9497\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5432 - categorical_crossentropy: 0.5432 - categorical_accuracy: 0.8314 - top_3_accuracy: 0.9497 - val_loss: 0.9940 - val_categorical_crossentropy: 0.9940 - val_categorical_accuracy: 0.7559 - val_top_3_accuracy: 0.8789\n",
            "Epoch 87/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.5280 - categorical_crossentropy: 0.5280 - categorical_accuracy: 0.8389 - top_3_accuracy: 0.9482 - val_loss: 1.0009 - val_categorical_crossentropy: 1.0009 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8770\n",
            "Epoch 88/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5314 - categorical_crossentropy: 0.5314 - categorical_accuracy: 0.8350 - top_3_accuracy: 0.9519 - val_loss: 0.9992 - val_categorical_crossentropy: 0.9992 - val_categorical_accuracy: 0.7598 - val_top_3_accuracy: 0.8770\n",
            "Epoch 89/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.5227 - categorical_crossentropy: 0.5227 - categorical_accuracy: 0.8374 - top_3_accuracy: 0.9517 - val_loss: 1.0001 - val_categorical_crossentropy: 1.0001 - val_categorical_accuracy: 0.7676 - val_top_3_accuracy: 0.8789\n",
            "Epoch 90/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5324 - categorical_crossentropy: 0.5324 - categorical_accuracy: 0.8347 - top_3_accuracy: 0.9498 - val_loss: 0.9912 - val_categorical_crossentropy: 0.9912 - val_categorical_accuracy: 0.7656 - val_top_3_accuracy: 0.8770\n",
            "Epoch 91/100\n",
            "218/218 [==============================] - 17s 80ms/step - loss: 0.5349 - categorical_crossentropy: 0.5349 - categorical_accuracy: 0.8319 - top_3_accuracy: 0.9503 - val_loss: 1.0040 - val_categorical_crossentropy: 1.0040 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8770\n",
            "Epoch 92/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.5150 - categorical_crossentropy: 0.5150 - categorical_accuracy: 0.8400 - top_3_accuracy: 0.9516 - val_loss: 0.9963 - val_categorical_crossentropy: 0.9963 - val_categorical_accuracy: 0.7598 - val_top_3_accuracy: 0.8809\n",
            "Epoch 93/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.5293 - categorical_crossentropy: 0.5293 - categorical_accuracy: 0.8338 - top_3_accuracy: 0.9508\n",
            "Epoch 00093: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5285 - categorical_crossentropy: 0.5285 - categorical_accuracy: 0.8340 - top_3_accuracy: 0.9510 - val_loss: 0.9985 - val_categorical_crossentropy: 0.9985 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8789\n",
            "Epoch 94/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.5237 - categorical_crossentropy: 0.5237 - categorical_accuracy: 0.8387 - top_3_accuracy: 0.9502 - val_loss: 0.9971 - val_categorical_crossentropy: 0.9971 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8770\n",
            "Epoch 95/100\n",
            "218/218 [==============================] - 16s 76ms/step - loss: 0.5210 - categorical_crossentropy: 0.5210 - categorical_accuracy: 0.8351 - top_3_accuracy: 0.9536 - val_loss: 0.9986 - val_categorical_crossentropy: 0.9986 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8789\n",
            "Epoch 96/100\n",
            "218/218 [==============================] - 17s 78ms/step - loss: 0.5296 - categorical_crossentropy: 0.5296 - categorical_accuracy: 0.8334 - top_3_accuracy: 0.9494 - val_loss: 0.9981 - val_categorical_crossentropy: 0.9981 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8789\n",
            "Epoch 97/100\n",
            "218/218 [==============================] - 16s 75ms/step - loss: 0.5207 - categorical_crossentropy: 0.5207 - categorical_accuracy: 0.8391 - top_3_accuracy: 0.9521 - val_loss: 0.9944 - val_categorical_crossentropy: 0.9944 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8789\n",
            "Epoch 98/100\n",
            "218/218 [==============================] - 16s 76ms/step - loss: 0.5212 - categorical_crossentropy: 0.5212 - categorical_accuracy: 0.8371 - top_3_accuracy: 0.9539 - val_loss: 0.9990 - val_categorical_crossentropy: 0.9990 - val_categorical_accuracy: 0.7598 - val_top_3_accuracy: 0.8789\n",
            "Epoch 99/100\n",
            "218/218 [==============================] - 17s 76ms/step - loss: 0.5275 - categorical_crossentropy: 0.5275 - categorical_accuracy: 0.8389 - top_3_accuracy: 0.9506 - val_loss: 0.9985 - val_categorical_crossentropy: 0.9985 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8789\n",
            "Epoch 100/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.5291 - categorical_crossentropy: 0.5291 - categorical_accuracy: 0.8363 - top_3_accuracy: 0.9509\n",
            "Epoch 00100: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "218/218 [==============================] - 17s 77ms/step - loss: 0.5283 - categorical_crossentropy: 0.5283 - categorical_accuracy: 0.8366 - top_3_accuracy: 0.9509 - val_loss: 1.0067 - val_categorical_crossentropy: 1.0067 - val_categorical_accuracy: 0.7617 - val_top_3_accuracy: 0.8770\n",
            "Test loss: 1.006652507930994\n",
            "Test accuracy: 1.006652507930994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hdkSZI2NrRAQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_cnn = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jW2SJXAu0Um",
        "colab_type": "code",
        "outputId": "de450925-0c11-4a26-bc2c-39c3a0ad1af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "test = []\n",
        "for c, img in enumerate(X_test):\n",
        "    img = img.reshape(100, 100)\n",
        "    test.append(remove_noise(img, 100).flatten())\n",
        "    \n",
        "for c, img in enumerate(test):\n",
        "    test[c] = np.array(img)\n",
        "test = np.array(test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:118: UserWarning: Possible sign loss when converting negative image of type float64 to positive image of type bool.\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to bool\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hW-CTFsrLpaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_cropped = center_image(test, X_test, show_image=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtCICMzUKlQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_x = X_test_cropped\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    test_x = test_x.reshape(test_x.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    test_x = test_x.reshape(test_x.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "test_x = test_x.astype('float32')\n",
        "test_x /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s5zJH74T1HaC",
        "colab_type": "code",
        "outputId": "259cee2f-2c7d-466e-b4ae-f18ce733fc7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "prediction_mn = model_mn.predict(test_x, batch_size=1, verbose=1)\n",
        "prediction_cnn = model_cnn.predict(test_x, batch_size=1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 68s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-07X1XH94bAi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sum_ = prediction_cnn + prediction_mn\n",
        "sum_ = np.argmax(sum_,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oHZ9Iqua4bHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels_df = pd.read_csv(path_to_labels)\n",
        "labels_df.Category = pd.Categorical(labels_df.Category)\n",
        "cat_map = pd.Categorical(labels_df.Category)\n",
        "cat_map = cat_map.categories\n",
        "result_sum = []\n",
        "for index, image in enumerate(sum_):\n",
        "  result_sum.append(cat_map[image])\n",
        "  \n",
        "result_sum = pd.DataFrame(result_sum)\n",
        "result_sum = result_sum.reset_index(drop=False)\n",
        "result_sum.columns=['Id', 'Category']\n",
        "result_sum.to_csv('result_sum.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gMQ7BlsMl59",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "result_cnn, result_mn = [], []\n",
        "labels_df = pd.read_csv(path_to_labels)\n",
        "labels_df.Category = pd.Categorical(labels_df.Category)\n",
        "cat_map = pd.Categorical(labels_df.Category)\n",
        "cat_map = cat_map.categories\n",
        "\n",
        "cnn_expected = np.argmax(prediction_cnn,axis=1)\n",
        "mn_expected = np.argmax(prediction_mn,axis=1)\n",
        "\n",
        "for index, image in enumerate(cnn_expected):\n",
        "  result_cnn.append(cat_map[image])\n",
        "\n",
        "result_cnn = pd.DataFrame(result_cnn)\n",
        "result_cnn = result_cnn.reset_index(drop=False)\n",
        "result_cnn.columns=['Id', 'Category']\n",
        "result_cnn.to_csv('result_cnn.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "for index, image in enumerate(mn_expected):\n",
        "  result_mn.append(cat_map[image])\n",
        "  \n",
        "result_mn = pd.DataFrame(result_mn)\n",
        "result_mn = result_mn.reset_index(drop=False)\n",
        "result_mn.columns=['Id', 'Category']\n",
        "result_mn.to_csv('result_mn.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}