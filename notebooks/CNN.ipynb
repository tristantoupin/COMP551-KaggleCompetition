{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "sDGhFDtdP_0K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import svm, metrics\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import random\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from scipy import stats\n",
        "import math\n",
        "from skimage import morphology, img_as_bool\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nt0OtpF6WXXa",
        "colab_type": "code",
        "outputId": "c52e5924-24d7-4544-d758-b56f4b7d5b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MuK3fO6SP_0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# path_to_data = \"gdrive/My Drive/data/train_images.npy\"\n",
        "# path_to_labels = \"gdrive/My Drive/data/train_labels.csv\"\n",
        "# path_to_test = \"gdrive/My Drive/data/test_images.npy\"\n",
        "\n",
        "path_broad = \"gdrive/My Drive/Mcgill/U4/Fall 2018/COMP 551/kaggle/data/\"\n",
        "path_to_data = path_broad + \"train_images.npy\"\n",
        "path_to_labels = path_broad + \"train_labels.csv\"\n",
        "path_to_test = path_broad + \"test_images.npy\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uA-4nzfSP_0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_images(list_of_images, max_col = 4):\n",
        "    n = len(list_of_images)\n",
        "    if n == 1:\n",
        "        plt.imshow(list_of_images[0], cmap=\"gray\"); plt.axis('off'); plt.show()\n",
        "    else:\n",
        "        # get number of columns and rows required\n",
        "        r, c = 1, n\n",
        "        if n > max_col:\n",
        "            c = max_col\n",
        "            r = int(math.ceil(n/max_col))\n",
        "    \n",
        "        fig = plt.figure(figsize=(20, max_col * r))\n",
        "        for i, (img,name) in enumerate(list_of_images):\n",
        "            ax = fig.add_subplot(r, c, (i+1))\n",
        "            ax.set_title(str(name))\n",
        "            ax.axis('off')\n",
        "            ax.imshow(img, cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZRn5lD6P_0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(path_x, path_y):\n",
        "    data = np.load(path_x, encoding = 'bytes')\n",
        "    labels_df = pd.read_csv(path_y)\n",
        "    labels_df.Category = pd.Categorical(labels_df.Category)\n",
        "    y = labels_df.Category.cat.codes.values\n",
        "    X = np.array(data[:, 1])\n",
        "    for c, i in enumerate(X):\n",
        "        ret,thresh_img = cv2.threshold(i,127,255,cv2.THRESH_BINARY)\n",
        "        X[c] = thresh_img\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VqFde78xP_0d",
        "colab_type": "code",
        "outputId": "2b8688b1-115e-457e-dbd6-75bf18bfbac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "X_noisy, y = get_data(path_to_data, path_to_labels)\n",
        "print(\"Shape of training dataset:\", len(X_noisy))\n",
        "X_test = list(np.load(path_to_test, encoding = 'bytes')[:, 1])\n",
        "print(\"Shape of testing dataset:\", len(X_test))\n",
        "unique_classes = set(y)\n",
        "print(\"Number of classes:\", len(unique_classes))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training dataset: 10000\n",
            "Shape of testing dataset: 10000\n",
            "Number of classes: 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RXrUpdFhP_0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def remove_noise(filledImg, min_size = 500):\n",
        "    blobs, min_val = ndimage.label(filledImg)\n",
        "    \n",
        "    for i in range(10,60):\n",
        "        clean_img = morphology.remove_small_objects(img_as_bool(filledImg), i)\n",
        "        blobs, number_of_blobs = ndimage.label(clean_img)\n",
        "        if number_of_blobs < min_val:\n",
        "            min_val = number_of_blobs\n",
        "            if min_val == 1:\n",
        "                return clean_img\n",
        "  \n",
        "    return clean_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TqY99QqOP_1a",
        "colab_type": "code",
        "outputId": "824dfd69-48ba-4a79-b533-a53b5adb69a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "X = []\n",
        "for c, (img, label) in enumerate(zip(X_noisy, y)):\n",
        "    img = img.reshape(100, 100)\n",
        "    X.append(remove_noise(img, 100).flatten())  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:118: UserWarning: Possible sign loss when converting negative image of type float64 to positive image of type bool.\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to bool\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0eXGh_j9P_1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for c, img in enumerate(X):\n",
        "    X[c] = np.array(img)\n",
        "X = np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64EcjwRFP_1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def crop_image(array_X, size_of_image = 100, size_of_crop = 32, show_image = False):\n",
        "  cropped = []\n",
        "  cropBox = None\n",
        "  for i in range(len(array_X)):\n",
        "    test = array_X[i]\n",
        "    test.resize(size_of_image,size_of_image)\n",
        "    image_data = test\n",
        "    if cropBox == None:\n",
        "      prev_cropBox = None\n",
        "    else:\n",
        "      prev_cropBox = cropBox[:]\n",
        "\n",
        "    try:  \n",
        "      non_empty_columns = np.where(image_data.max(axis=0)>0)[0]\n",
        "      non_empty_rows = np.where(image_data.max(axis=1)>0)[0]\n",
        "      cropBox = (min(non_empty_rows), max(non_empty_rows), min(non_empty_columns), max(non_empty_columns))\n",
        "    except:\n",
        "      cropBox = prev_cropBox\n",
        "    shiftX = size_of_crop\n",
        "    shiftY = size_of_crop\n",
        "    cropBox = list(cropBox)\n",
        "    if(cropBox[0] >= 5):\n",
        "      cropBox[0] -= 5\n",
        "\n",
        "    if(cropBox[0]+shiftX >= size_of_image):\n",
        "      cropBox[0] = size_of_image - shiftX - 1\n",
        "\n",
        "    if(cropBox[2] >= 5):\n",
        "      cropBox[2] -= 5\n",
        "\n",
        "    if(cropBox[2]+shiftY >= size_of_image):\n",
        "      cropBox[2] = size_of_image - shiftY - 1\n",
        "\n",
        "    image_data_new = image_data[cropBox[0]:cropBox[0]+shiftX, cropBox[2]:cropBox[2]+shiftY]\n",
        "    \n",
        "    if show_image:\n",
        "      plot_images(np.array([image_data_new]))\n",
        "\n",
        "    cropped.append(image_data_new.flatten())\n",
        "  \n",
        "  for c, img in enumerate(cropped):\n",
        "    cropped[c] = np.array(img)\n",
        "  cropped = np.array(cropped)\n",
        "  return cropped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8G62U9lP_11",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_cropped = crop_image(X, size_of_crop=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4xkhRKcku6J",
        "colab_type": "code",
        "outputId": "6f3b374d-c14a-4dd5-bc0a-32e995c51330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3454
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 31\n",
        "epochs = 35\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 32, 32\n",
        "# the data, split between train and val sets\n",
        "(x_train, y_train), (x_val, y_val) = (X_cropped[:9488],y[:9488]), (X_cropped[9488:],y[9488:]) \n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_val.shape[0], 'val samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "print(y_train[3])\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=False,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "datagen.fit(x_train)\n",
        "val_datagen = ImageDataGenerator()\n",
        "val_datagen.fit(x_val)\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "validation_generator = val_datagen.flow(x_val, y_val, batch_size=batch_size)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu',input_shape=input_shape))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=0.002,\n",
        "                  beta_1=0.9,\n",
        "                  beta_2=0.999,\n",
        "                  epsilon=1e-08,\n",
        "                  schedule_decay=0.004)\n",
        "\n",
        "# Set our optimizer and loss function (similar settings to our CAE approach)\n",
        "model.compile(loss = keras.losses.categorical_crossentropy,\n",
        "            optimizer = optimizer,\n",
        "            metrics = ['categorical_accuracy'])\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_categorical_accuracy', \n",
        "                      factor=0.1, patience=5,\n",
        "                      min_delta=0.0001, mode='auto',\n",
        "                      cooldown=0, verbose=1, \n",
        "                      min_lr=0)\n",
        "]\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "          steps_per_epoch= 14000 // batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1, \n",
        "          validation_data=(x_val, y_val),\n",
        "          callbacks = callbacks\n",
        "          )\n",
        "\n",
        "\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (9488, 32, 32, 1)\n",
            "9488 train samples\n",
            "512 val samples\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 1/85\n",
            "218/218 [==============================] - 11s 52ms/step - loss: 2.7486 - categorical_accuracy: 0.2144 - val_loss: 1.9055 - val_categorical_accuracy: 0.4277\n",
            "Epoch 2/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 2.2419 - categorical_accuracy: 0.3498 - val_loss: 1.6782 - val_categorical_accuracy: 0.5078\n",
            "Epoch 3/85\n",
            "218/218 [==============================] - 8s 35ms/step - loss: 2.0182 - categorical_accuracy: 0.4197 - val_loss: 1.4887 - val_categorical_accuracy: 0.5645\n",
            "Epoch 4/85\n",
            "218/218 [==============================] - 8s 35ms/step - loss: 1.8654 - categorical_accuracy: 0.4605 - val_loss: 1.3380 - val_categorical_accuracy: 0.6152\n",
            "Epoch 5/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.7688 - categorical_accuracy: 0.4907 - val_loss: 1.2972 - val_categorical_accuracy: 0.6367\n",
            "Epoch 6/85\n",
            "218/218 [==============================] - 8s 37ms/step - loss: 1.6785 - categorical_accuracy: 0.5204 - val_loss: 1.2920 - val_categorical_accuracy: 0.6230\n",
            "Epoch 7/85\n",
            "218/218 [==============================] - 8s 36ms/step - loss: 1.6144 - categorical_accuracy: 0.5340 - val_loss: 1.2404 - val_categorical_accuracy: 0.6719\n",
            "Epoch 8/85\n",
            "218/218 [==============================] - 8s 36ms/step - loss: 1.5852 - categorical_accuracy: 0.5488 - val_loss: 1.1292 - val_categorical_accuracy: 0.6875\n",
            "Epoch 9/85\n",
            "218/218 [==============================] - 8s 36ms/step - loss: 1.5310 - categorical_accuracy: 0.5624 - val_loss: 1.0890 - val_categorical_accuracy: 0.6777\n",
            "Epoch 10/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.4780 - categorical_accuracy: 0.5798 - val_loss: 1.0384 - val_categorical_accuracy: 0.7227\n",
            "Epoch 11/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.4543 - categorical_accuracy: 0.5768 - val_loss: 1.0594 - val_categorical_accuracy: 0.6836\n",
            "Epoch 12/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.4290 - categorical_accuracy: 0.5927 - val_loss: 1.0508 - val_categorical_accuracy: 0.7109\n",
            "Epoch 13/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.3996 - categorical_accuracy: 0.6024 - val_loss: 1.0047 - val_categorical_accuracy: 0.7129\n",
            "Epoch 14/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.4009 - categorical_accuracy: 0.5957 - val_loss: 1.0365 - val_categorical_accuracy: 0.7129\n",
            "Epoch 15/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.3822 - categorical_accuracy: 0.6040 - val_loss: 0.9923 - val_categorical_accuracy: 0.7129\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 16/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.2960 - categorical_accuracy: 0.6317 - val_loss: 0.9538 - val_categorical_accuracy: 0.7246\n",
            "Epoch 17/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.2614 - categorical_accuracy: 0.6406 - val_loss: 0.9184 - val_categorical_accuracy: 0.7363\n",
            "Epoch 18/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.2140 - categorical_accuracy: 0.6469 - val_loss: 0.8977 - val_categorical_accuracy: 0.7363\n",
            "Epoch 19/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.2263 - categorical_accuracy: 0.6454 - val_loss: 0.8978 - val_categorical_accuracy: 0.7441\n",
            "Epoch 20/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1967 - categorical_accuracy: 0.6526 - val_loss: 0.8906 - val_categorical_accuracy: 0.7402\n",
            "Epoch 21/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1968 - categorical_accuracy: 0.6581 - val_loss: 0.8898 - val_categorical_accuracy: 0.7520\n",
            "Epoch 22/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1677 - categorical_accuracy: 0.6630 - val_loss: 0.8725 - val_categorical_accuracy: 0.7539\n",
            "Epoch 23/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1696 - categorical_accuracy: 0.6623 - val_loss: 0.8614 - val_categorical_accuracy: 0.7617\n",
            "Epoch 24/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1754 - categorical_accuracy: 0.6623 - val_loss: 0.8705 - val_categorical_accuracy: 0.7559\n",
            "Epoch 25/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1682 - categorical_accuracy: 0.6662 - val_loss: 0.8647 - val_categorical_accuracy: 0.7617\n",
            "Epoch 26/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1452 - categorical_accuracy: 0.6679 - val_loss: 0.8512 - val_categorical_accuracy: 0.7520\n",
            "Epoch 27/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1473 - categorical_accuracy: 0.6685 - val_loss: 0.8564 - val_categorical_accuracy: 0.7480\n",
            "Epoch 28/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1473 - categorical_accuracy: 0.6733 - val_loss: 0.8409 - val_categorical_accuracy: 0.7637\n",
            "Epoch 29/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1402 - categorical_accuracy: 0.6667 - val_loss: 0.8648 - val_categorical_accuracy: 0.7539\n",
            "Epoch 30/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1223 - categorical_accuracy: 0.6769 - val_loss: 0.8427 - val_categorical_accuracy: 0.7598\n",
            "Epoch 31/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1389 - categorical_accuracy: 0.6680 - val_loss: 0.8436 - val_categorical_accuracy: 0.7598\n",
            "Epoch 32/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1136 - categorical_accuracy: 0.6800 - val_loss: 0.8574 - val_categorical_accuracy: 0.7598\n",
            "Epoch 33/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1120 - categorical_accuracy: 0.6809 - val_loss: 0.8564 - val_categorical_accuracy: 0.7617\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
            "Epoch 34/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0983 - categorical_accuracy: 0.6840 - val_loss: 0.8507 - val_categorical_accuracy: 0.7734\n",
            "Epoch 35/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1026 - categorical_accuracy: 0.6812 - val_loss: 0.8498 - val_categorical_accuracy: 0.7715\n",
            "Epoch 36/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1026 - categorical_accuracy: 0.6823 - val_loss: 0.8463 - val_categorical_accuracy: 0.7734\n",
            "Epoch 37/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1069 - categorical_accuracy: 0.6767 - val_loss: 0.8466 - val_categorical_accuracy: 0.7695\n",
            "Epoch 38/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1033 - categorical_accuracy: 0.6787 - val_loss: 0.8442 - val_categorical_accuracy: 0.7656\n",
            "Epoch 39/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0965 - categorical_accuracy: 0.6796 - val_loss: 0.8464 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
            "Epoch 40/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0862 - categorical_accuracy: 0.6838 - val_loss: 0.8462 - val_categorical_accuracy: 0.7656\n",
            "Epoch 41/85\n",
            "218/218 [==============================] - 7s 34ms/step - loss: 1.1001 - categorical_accuracy: 0.6801 - val_loss: 0.8451 - val_categorical_accuracy: 0.7656\n",
            "Epoch 42/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1001 - categorical_accuracy: 0.6826 - val_loss: 0.8448 - val_categorical_accuracy: 0.7656\n",
            "Epoch 43/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1007 - categorical_accuracy: 0.6778 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 44/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0911 - categorical_accuracy: 0.6843 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 2.000000222324161e-07.\n",
            "Epoch 45/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1048 - categorical_accuracy: 0.6805 - val_loss: 0.8438 - val_categorical_accuracy: 0.7656\n",
            "Epoch 46/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.1138 - categorical_accuracy: 0.6816 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 47/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0950 - categorical_accuracy: 0.6800 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 48/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1028 - categorical_accuracy: 0.6770 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 49/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0849 - categorical_accuracy: 0.6871 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-08.\n",
            "Epoch 50/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.1012 - categorical_accuracy: 0.6817 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 51/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0995 - categorical_accuracy: 0.6800 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 52/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0829 - categorical_accuracy: 0.6846 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 53/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.1002 - categorical_accuracy: 0.6826 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 54/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0923 - categorical_accuracy: 0.6851 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-09.\n",
            "Epoch 55/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0860 - categorical_accuracy: 0.6856 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 56/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0820 - categorical_accuracy: 0.6842 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 57/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0852 - categorical_accuracy: 0.6814 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 58/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0960 - categorical_accuracy: 0.6839 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 59/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0919 - categorical_accuracy: 0.6818 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-10.\n",
            "Epoch 60/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.1023 - categorical_accuracy: 0.6798 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 61/85\n",
            "218/218 [==============================] - 7s 31ms/step - loss: 1.0986 - categorical_accuracy: 0.6807 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 62/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0946 - categorical_accuracy: 0.6843 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 63/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0973 - categorical_accuracy: 0.6806 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 64/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.0837 - categorical_accuracy: 0.6840 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-11.\n",
            "Epoch 65/85\n",
            "218/218 [==============================] - 7s 32ms/step - loss: 1.1031 - categorical_accuracy: 0.6809 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 66/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0949 - categorical_accuracy: 0.6830 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 67/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0861 - categorical_accuracy: 0.6851 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 68/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0829 - categorical_accuracy: 0.6855 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 69/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0868 - categorical_accuracy: 0.6830 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-12.\n",
            "Epoch 70/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1002 - categorical_accuracy: 0.6807 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 71/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0753 - categorical_accuracy: 0.6884 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 72/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1051 - categorical_accuracy: 0.6839 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 73/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0915 - categorical_accuracy: 0.6824 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 74/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0981 - categorical_accuracy: 0.6833 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 2.000000208848829e-13.\n",
            "Epoch 75/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1054 - categorical_accuracy: 0.6783 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 76/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1047 - categorical_accuracy: 0.6840 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 77/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1173 - categorical_accuracy: 0.6760 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 78/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0840 - categorical_accuracy: 0.6833 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 79/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0928 - categorical_accuracy: 0.6844 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 2.0000002359538835e-14.\n",
            "Epoch 80/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1012 - categorical_accuracy: 0.6821 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 81/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0705 - categorical_accuracy: 0.6907 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 82/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.1132 - categorical_accuracy: 0.6803 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 83/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0990 - categorical_accuracy: 0.6782 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Epoch 84/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0953 - categorical_accuracy: 0.6810 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 2.000000303716519e-15.\n",
            "Epoch 85/85\n",
            "218/218 [==============================] - 7s 33ms/step - loss: 1.0914 - categorical_accuracy: 0.6817 - val_loss: 0.8439 - val_categorical_accuracy: 0.7656\n",
            "Test loss: 0.8438710011541843\n",
            "Test accuracy: 0.765625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ldA0n91xrQ7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3896
        },
        "outputId": "6294250a-ae43-4d16-91de-31cd0543b372"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "\n",
        "def top_3_accuracy(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "  \n",
        "  \n",
        "batch_size = 64\n",
        "num_classes = 31\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 32, 32\n",
        "# the data, split between train and val sets\n",
        "(x_train, y_train), (x_val, y_val) = (X_cropped[:9488],y[:9488]), (X_cropped[9488:],y[9488:]) \n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_val.shape[0], 'val samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "print(y_train[3])\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=False,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "datagen.fit(x_train)\n",
        "val_datagen = ImageDataGenerator()\n",
        "val_datagen.fit(x_val)\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "validation_generator = val_datagen.flow(x_val, y_val, batch_size=batch_size)\n",
        "\n",
        "model_mn = MobileNet(input_shape=(img_rows, img_cols, 1), alpha=1., weights=None, classes=num_classes)\n",
        "model_mn.compile(optimizer=Adam(lr=0.002), loss='categorical_crossentropy',\n",
        "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\n",
        "\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5,\n",
        "                      min_delta=0.005, mode='max', cooldown=3, verbose=1)\n",
        "]\n",
        "\n",
        "hist = model_mn.fit_generator(\n",
        "    train_generator, steps_per_epoch= 14000 // batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks = callbacks\n",
        ")\n",
        "\n",
        "score = model_mn.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (9488, 32, 32, 1)\n",
            "9488 train samples\n",
            "512 val samples\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 1/100\n",
            "218/218 [==============================] - 19s 89ms/step - loss: 3.0663 - categorical_crossentropy: 3.0663 - categorical_accuracy: 0.1593 - top_3_accuracy: 0.3572 - val_loss: 3.4743 - val_categorical_crossentropy: 3.4743 - val_categorical_accuracy: 0.0566 - val_top_3_accuracy: 0.1230\n",
            "Epoch 2/100\n",
            "218/218 [==============================] - 14s 62ms/step - loss: 2.6984 - categorical_crossentropy: 2.6984 - categorical_accuracy: 0.2413 - top_3_accuracy: 0.4965 - val_loss: 3.5479 - val_categorical_crossentropy: 3.5479 - val_categorical_accuracy: 0.0566 - val_top_3_accuracy: 0.1328\n",
            "Epoch 3/100\n",
            "218/218 [==============================] - 14s 62ms/step - loss: 2.5586 - categorical_crossentropy: 2.5586 - categorical_accuracy: 0.2876 - top_3_accuracy: 0.5452 - val_loss: 2.9849 - val_categorical_crossentropy: 2.9849 - val_categorical_accuracy: 0.1094 - val_top_3_accuracy: 0.3145\n",
            "Epoch 4/100\n",
            "218/218 [==============================] - 14s 62ms/step - loss: 2.5072 - categorical_crossentropy: 2.5072 - categorical_accuracy: 0.3053 - top_3_accuracy: 0.5754 - val_loss: 6.6869 - val_categorical_crossentropy: 6.6869 - val_categorical_accuracy: 0.0898 - val_top_3_accuracy: 0.2520\n",
            "Epoch 5/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 2.3624 - categorical_crossentropy: 2.3624 - categorical_accuracy: 0.3500 - top_3_accuracy: 0.6099 - val_loss: 10.4076 - val_categorical_crossentropy: 10.4076 - val_categorical_accuracy: 0.1172 - val_top_3_accuracy: 0.2441\n",
            "Epoch 6/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 2.2882 - categorical_crossentropy: 2.2882 - categorical_accuracy: 0.3746 - top_3_accuracy: 0.6361 - val_loss: 3.4366 - val_categorical_crossentropy: 3.4366 - val_categorical_accuracy: 0.3125 - val_top_3_accuracy: 0.5762\n",
            "Epoch 7/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 2.1550 - categorical_crossentropy: 2.1550 - categorical_accuracy: 0.4055 - top_3_accuracy: 0.6733 - val_loss: 2.4872 - val_categorical_crossentropy: 2.4872 - val_categorical_accuracy: 0.4023 - val_top_3_accuracy: 0.6230\n",
            "Epoch 8/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 2.2220 - categorical_crossentropy: 2.2220 - categorical_accuracy: 0.3938 - top_3_accuracy: 0.6522 - val_loss: 3.3552 - val_categorical_crossentropy: 3.3552 - val_categorical_accuracy: 0.3574 - val_top_3_accuracy: 0.6016\n",
            "Epoch 9/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 2.4076 - categorical_crossentropy: 2.4076 - categorical_accuracy: 0.3610 - top_3_accuracy: 0.6186 - val_loss: 10.0814 - val_categorical_crossentropy: 10.0814 - val_categorical_accuracy: 0.0938 - val_top_3_accuracy: 0.2070\n",
            "Epoch 10/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 2.3476 - categorical_crossentropy: 2.3476 - categorical_accuracy: 0.3544 - top_3_accuracy: 0.6281 - val_loss: 7.2032 - val_categorical_crossentropy: 7.2032 - val_categorical_accuracy: 0.1250 - val_top_3_accuracy: 0.2793\n",
            "Epoch 11/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 2.3081 - categorical_crossentropy: 2.3081 - categorical_accuracy: 0.3538 - top_3_accuracy: 0.6208 - val_loss: 1.9444 - val_categorical_crossentropy: 1.9444 - val_categorical_accuracy: 0.4238 - val_top_3_accuracy: 0.6836\n",
            "Epoch 12/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.9573 - categorical_crossentropy: 1.9573 - categorical_accuracy: 0.4204 - top_3_accuracy: 0.6873 - val_loss: 1.6925 - val_categorical_crossentropy: 1.6925 - val_categorical_accuracy: 0.5098 - val_top_3_accuracy: 0.7559\n",
            "Epoch 13/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.8452 - categorical_crossentropy: 1.8452 - categorical_accuracy: 0.4538 - top_3_accuracy: 0.7155 - val_loss: 1.6737 - val_categorical_crossentropy: 1.6737 - val_categorical_accuracy: 0.5215 - val_top_3_accuracy: 0.7910\n",
            "Epoch 14/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.7780 - categorical_crossentropy: 1.7780 - categorical_accuracy: 0.4720 - top_3_accuracy: 0.7327 - val_loss: 1.6874 - val_categorical_crossentropy: 1.6874 - val_categorical_accuracy: 0.5078 - val_top_3_accuracy: 0.7480\n",
            "Epoch 15/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.7585 - categorical_crossentropy: 1.7585 - categorical_accuracy: 0.4819 - top_3_accuracy: 0.7362 - val_loss: 1.5082 - val_categorical_crossentropy: 1.5082 - val_categorical_accuracy: 0.5645 - val_top_3_accuracy: 0.7930\n",
            "Epoch 16/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 1.7193 - categorical_crossentropy: 1.7193 - categorical_accuracy: 0.4962 - top_3_accuracy: 0.7457 - val_loss: 1.5595 - val_categorical_crossentropy: 1.5595 - val_categorical_accuracy: 0.5723 - val_top_3_accuracy: 0.7676\n",
            "Epoch 17/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.6731 - categorical_crossentropy: 1.6731 - categorical_accuracy: 0.5044 - top_3_accuracy: 0.7521 - val_loss: 1.4579 - val_categorical_crossentropy: 1.4579 - val_categorical_accuracy: 0.5820 - val_top_3_accuracy: 0.8164\n",
            "Epoch 18/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.6414 - categorical_crossentropy: 1.6414 - categorical_accuracy: 0.5182 - top_3_accuracy: 0.7617 - val_loss: 1.6082 - val_categorical_crossentropy: 1.6082 - val_categorical_accuracy: 0.5605 - val_top_3_accuracy: 0.7734\n",
            "Epoch 19/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.6221 - categorical_crossentropy: 1.6221 - categorical_accuracy: 0.5252 - top_3_accuracy: 0.7623 - val_loss: 1.5789 - val_categorical_crossentropy: 1.5789 - val_categorical_accuracy: 0.5742 - val_top_3_accuracy: 0.7773\n",
            "Epoch 20/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 1.6274 - categorical_crossentropy: 1.6274 - categorical_accuracy: 0.5233 - top_3_accuracy: 0.7638 - val_loss: 1.9743 - val_categorical_crossentropy: 1.9743 - val_categorical_accuracy: 0.4648 - val_top_3_accuracy: 0.6875\n",
            "Epoch 21/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 1.5944 - categorical_crossentropy: 1.5944 - categorical_accuracy: 0.5339 - top_3_accuracy: 0.7714 - val_loss: 1.5357 - val_categorical_crossentropy: 1.5357 - val_categorical_accuracy: 0.5742 - val_top_3_accuracy: 0.7754\n",
            "Epoch 22/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 1.5534 - categorical_crossentropy: 1.5534 - categorical_accuracy: 0.5482 - top_3_accuracy: 0.7787\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.5540 - categorical_crossentropy: 1.5540 - categorical_accuracy: 0.5479 - top_3_accuracy: 0.7788 - val_loss: 1.6339 - val_categorical_crossentropy: 1.6339 - val_categorical_accuracy: 0.5762 - val_top_3_accuracy: 0.7715\n",
            "Epoch 23/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 1.4587 - categorical_crossentropy: 1.4587 - categorical_accuracy: 0.5743 - top_3_accuracy: 0.7990 - val_loss: 1.4125 - val_categorical_crossentropy: 1.4125 - val_categorical_accuracy: 0.5859 - val_top_3_accuracy: 0.8047\n",
            "Epoch 24/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 1.4047 - categorical_crossentropy: 1.4047 - categorical_accuracy: 0.5868 - top_3_accuracy: 0.8083 - val_loss: 1.3922 - val_categorical_crossentropy: 1.3922 - val_categorical_accuracy: 0.6074 - val_top_3_accuracy: 0.8086\n",
            "Epoch 25/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.3921 - categorical_crossentropy: 1.3921 - categorical_accuracy: 0.5932 - top_3_accuracy: 0.8111 - val_loss: 1.2699 - val_categorical_crossentropy: 1.2699 - val_categorical_accuracy: 0.6387 - val_top_3_accuracy: 0.8223\n",
            "Epoch 26/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.3686 - categorical_crossentropy: 1.3686 - categorical_accuracy: 0.6001 - top_3_accuracy: 0.8185 - val_loss: 1.4973 - val_categorical_crossentropy: 1.4973 - val_categorical_accuracy: 0.5977 - val_top_3_accuracy: 0.8125\n",
            "Epoch 27/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.3537 - categorical_crossentropy: 1.3537 - categorical_accuracy: 0.6049 - top_3_accuracy: 0.8154 - val_loss: 1.4185 - val_categorical_crossentropy: 1.4185 - val_categorical_accuracy: 0.6094 - val_top_3_accuracy: 0.8105\n",
            "Epoch 28/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.3349 - categorical_crossentropy: 1.3349 - categorical_accuracy: 0.6078 - top_3_accuracy: 0.8205 - val_loss: 1.3556 - val_categorical_crossentropy: 1.3556 - val_categorical_accuracy: 0.6211 - val_top_3_accuracy: 0.8105\n",
            "Epoch 29/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.3097 - categorical_crossentropy: 1.3097 - categorical_accuracy: 0.6137 - top_3_accuracy: 0.8287 - val_loss: 1.4476 - val_categorical_crossentropy: 1.4476 - val_categorical_accuracy: 0.5859 - val_top_3_accuracy: 0.8047\n",
            "Epoch 30/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 1.3103 - categorical_crossentropy: 1.3103 - categorical_accuracy: 0.6148 - top_3_accuracy: 0.8314\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.3104 - categorical_crossentropy: 1.3104 - categorical_accuracy: 0.6148 - top_3_accuracy: 0.8316 - val_loss: 1.4322 - val_categorical_crossentropy: 1.4322 - val_categorical_accuracy: 0.5996 - val_top_3_accuracy: 0.8281\n",
            "Epoch 31/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.2636 - categorical_crossentropy: 1.2636 - categorical_accuracy: 0.6266 - top_3_accuracy: 0.8352 - val_loss: 1.2630 - val_categorical_crossentropy: 1.2630 - val_categorical_accuracy: 0.6504 - val_top_3_accuracy: 0.8477\n",
            "Epoch 32/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.2065 - categorical_crossentropy: 1.2065 - categorical_accuracy: 0.6459 - top_3_accuracy: 0.8443 - val_loss: 1.2432 - val_categorical_crossentropy: 1.2432 - val_categorical_accuracy: 0.6523 - val_top_3_accuracy: 0.8398\n",
            "Epoch 33/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.2485 - categorical_crossentropy: 1.2485 - categorical_accuracy: 0.6313 - top_3_accuracy: 0.8381 - val_loss: 1.1363 - val_categorical_crossentropy: 1.1363 - val_categorical_accuracy: 0.6816 - val_top_3_accuracy: 0.8516\n",
            "Epoch 34/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.2009 - categorical_crossentropy: 1.2009 - categorical_accuracy: 0.6502 - top_3_accuracy: 0.8465 - val_loss: 1.2122 - val_categorical_crossentropy: 1.2122 - val_categorical_accuracy: 0.6602 - val_top_3_accuracy: 0.8477\n",
            "Epoch 35/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.1703 - categorical_crossentropy: 1.1703 - categorical_accuracy: 0.6560 - top_3_accuracy: 0.8511 - val_loss: 1.1452 - val_categorical_crossentropy: 1.1452 - val_categorical_accuracy: 0.6875 - val_top_3_accuracy: 0.8340\n",
            "Epoch 36/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.1714 - categorical_crossentropy: 1.1714 - categorical_accuracy: 0.6552 - top_3_accuracy: 0.8500 - val_loss: 1.1920 - val_categorical_crossentropy: 1.1920 - val_categorical_accuracy: 0.6855 - val_top_3_accuracy: 0.8340\n",
            "Epoch 37/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.1427 - categorical_crossentropy: 1.1427 - categorical_accuracy: 0.6598 - top_3_accuracy: 0.8557 - val_loss: 1.1369 - val_categorical_crossentropy: 1.1369 - val_categorical_accuracy: 0.6914 - val_top_3_accuracy: 0.8516\n",
            "Epoch 38/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.1480 - categorical_crossentropy: 1.1480 - categorical_accuracy: 0.6645 - top_3_accuracy: 0.8550 - val_loss: 1.1677 - val_categorical_crossentropy: 1.1677 - val_categorical_accuracy: 0.6875 - val_top_3_accuracy: 0.8477\n",
            "Epoch 39/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.1375 - categorical_crossentropy: 1.1375 - categorical_accuracy: 0.6600 - top_3_accuracy: 0.8576 - val_loss: 1.1459 - val_categorical_crossentropy: 1.1459 - val_categorical_accuracy: 0.6797 - val_top_3_accuracy: 0.8438\n",
            "Epoch 40/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 1.1230 - categorical_crossentropy: 1.1230 - categorical_accuracy: 0.6691 - top_3_accuracy: 0.8595\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "218/218 [==============================] - 15s 67ms/step - loss: 1.1233 - categorical_crossentropy: 1.1233 - categorical_accuracy: 0.6693 - top_3_accuracy: 0.8592 - val_loss: 1.2391 - val_categorical_crossentropy: 1.2391 - val_categorical_accuracy: 0.6602 - val_top_3_accuracy: 0.8438\n",
            "Epoch 41/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.0884 - categorical_crossentropy: 1.0884 - categorical_accuracy: 0.6796 - top_3_accuracy: 0.8648 - val_loss: 1.1320 - val_categorical_crossentropy: 1.1320 - val_categorical_accuracy: 0.6797 - val_top_3_accuracy: 0.8516\n",
            "Epoch 42/100\n",
            "218/218 [==============================] - 15s 68ms/step - loss: 1.0615 - categorical_crossentropy: 1.0615 - categorical_accuracy: 0.6836 - top_3_accuracy: 0.8681 - val_loss: 1.1255 - val_categorical_crossentropy: 1.1255 - val_categorical_accuracy: 0.6875 - val_top_3_accuracy: 0.8359\n",
            "Epoch 43/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.0737 - categorical_crossentropy: 1.0737 - categorical_accuracy: 0.6821 - top_3_accuracy: 0.8683 - val_loss: 1.0751 - val_categorical_crossentropy: 1.0751 - val_categorical_accuracy: 0.6953 - val_top_3_accuracy: 0.8516\n",
            "Epoch 44/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.0414 - categorical_crossentropy: 1.0414 - categorical_accuracy: 0.6902 - top_3_accuracy: 0.8753 - val_loss: 1.0803 - val_categorical_crossentropy: 1.0803 - val_categorical_accuracy: 0.6973 - val_top_3_accuracy: 0.8555\n",
            "Epoch 45/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.0373 - categorical_crossentropy: 1.0373 - categorical_accuracy: 0.6908 - top_3_accuracy: 0.8751 - val_loss: 1.0954 - val_categorical_crossentropy: 1.0954 - val_categorical_accuracy: 0.7051 - val_top_3_accuracy: 0.8574\n",
            "Epoch 46/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 1.0165 - categorical_crossentropy: 1.0165 - categorical_accuracy: 0.7011 - top_3_accuracy: 0.8741 - val_loss: 1.1339 - val_categorical_crossentropy: 1.1339 - val_categorical_accuracy: 0.6934 - val_top_3_accuracy: 0.8535\n",
            "Epoch 47/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.0293 - categorical_crossentropy: 1.0293 - categorical_accuracy: 0.6942 - top_3_accuracy: 0.8759 - val_loss: 1.1357 - val_categorical_crossentropy: 1.1357 - val_categorical_accuracy: 0.6758 - val_top_3_accuracy: 0.8555\n",
            "Epoch 48/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.0203 - categorical_crossentropy: 1.0203 - categorical_accuracy: 0.6936 - top_3_accuracy: 0.8787 - val_loss: 1.1110 - val_categorical_crossentropy: 1.1110 - val_categorical_accuracy: 0.6973 - val_top_3_accuracy: 0.8594\n",
            "Epoch 49/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.0115 - categorical_crossentropy: 1.0115 - categorical_accuracy: 0.6999 - top_3_accuracy: 0.8771 - val_loss: 1.1132 - val_categorical_crossentropy: 1.1132 - val_categorical_accuracy: 0.6914 - val_top_3_accuracy: 0.8438\n",
            "Epoch 50/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 1.0088 - categorical_crossentropy: 1.0088 - categorical_accuracy: 0.7004 - top_3_accuracy: 0.8771\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 1.0095 - categorical_crossentropy: 1.0095 - categorical_accuracy: 0.7003 - top_3_accuracy: 0.8769 - val_loss: 1.1587 - val_categorical_crossentropy: 1.1587 - val_categorical_accuracy: 0.6699 - val_top_3_accuracy: 0.8496\n",
            "Epoch 51/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9816 - categorical_crossentropy: 0.9816 - categorical_accuracy: 0.7061 - top_3_accuracy: 0.8835 - val_loss: 1.0329 - val_categorical_crossentropy: 1.0329 - val_categorical_accuracy: 0.7109 - val_top_3_accuracy: 0.8672\n",
            "Epoch 52/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.9730 - categorical_crossentropy: 0.9730 - categorical_accuracy: 0.7091 - top_3_accuracy: 0.8842 - val_loss: 1.0867 - val_categorical_crossentropy: 1.0867 - val_categorical_accuracy: 0.7090 - val_top_3_accuracy: 0.8555\n",
            "Epoch 53/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.9739 - categorical_crossentropy: 0.9739 - categorical_accuracy: 0.7046 - top_3_accuracy: 0.8856 - val_loss: 1.0643 - val_categorical_crossentropy: 1.0643 - val_categorical_accuracy: 0.7148 - val_top_3_accuracy: 0.8613\n",
            "Epoch 54/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.9639 - categorical_crossentropy: 0.9639 - categorical_accuracy: 0.7150 - top_3_accuracy: 0.8855 - val_loss: 1.0723 - val_categorical_crossentropy: 1.0723 - val_categorical_accuracy: 0.7109 - val_top_3_accuracy: 0.8652\n",
            "Epoch 55/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9619 - categorical_crossentropy: 0.9619 - categorical_accuracy: 0.7109 - top_3_accuracy: 0.8850 - val_loss: 1.0847 - val_categorical_crossentropy: 1.0847 - val_categorical_accuracy: 0.7031 - val_top_3_accuracy: 0.8594\n",
            "Epoch 56/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9473 - categorical_crossentropy: 0.9473 - categorical_accuracy: 0.7179 - top_3_accuracy: 0.8879 - val_loss: 1.0518 - val_categorical_crossentropy: 1.0518 - val_categorical_accuracy: 0.7148 - val_top_3_accuracy: 0.8691\n",
            "Epoch 57/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9400 - categorical_crossentropy: 0.9400 - categorical_accuracy: 0.7181 - top_3_accuracy: 0.8903 - val_loss: 1.0449 - val_categorical_crossentropy: 1.0449 - val_categorical_accuracy: 0.7246 - val_top_3_accuracy: 0.8613\n",
            "Epoch 58/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9503 - categorical_crossentropy: 0.9503 - categorical_accuracy: 0.7142 - top_3_accuracy: 0.8862 - val_loss: 1.0533 - val_categorical_crossentropy: 1.0533 - val_categorical_accuracy: 0.7109 - val_top_3_accuracy: 0.8691\n",
            "Epoch 59/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 0.9358 - categorical_crossentropy: 0.9358 - categorical_accuracy: 0.7162 - top_3_accuracy: 0.8893 - val_loss: 1.0847 - val_categorical_crossentropy: 1.0847 - val_categorical_accuracy: 0.7012 - val_top_3_accuracy: 0.8633\n",
            "Epoch 60/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.9520 - categorical_crossentropy: 0.9520 - categorical_accuracy: 0.7167 - top_3_accuracy: 0.8839 - val_loss: 1.0587 - val_categorical_crossentropy: 1.0587 - val_categorical_accuracy: 0.7051 - val_top_3_accuracy: 0.8672\n",
            "Epoch 61/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9579 - categorical_crossentropy: 0.9579 - categorical_accuracy: 0.7117 - top_3_accuracy: 0.8872 - val_loss: 1.0743 - val_categorical_crossentropy: 1.0743 - val_categorical_accuracy: 0.7090 - val_top_3_accuracy: 0.8574\n",
            "Epoch 62/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.9241 - categorical_crossentropy: 0.9241 - categorical_accuracy: 0.7241 - top_3_accuracy: 0.8908\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 0.9243 - categorical_crossentropy: 0.9243 - categorical_accuracy: 0.7238 - top_3_accuracy: 0.8909 - val_loss: 1.0603 - val_categorical_crossentropy: 1.0603 - val_categorical_accuracy: 0.7051 - val_top_3_accuracy: 0.8672\n",
            "Epoch 63/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9267 - categorical_crossentropy: 0.9267 - categorical_accuracy: 0.7245 - top_3_accuracy: 0.8939 - val_loss: 1.0783 - val_categorical_crossentropy: 1.0783 - val_categorical_accuracy: 0.7090 - val_top_3_accuracy: 0.8633\n",
            "Epoch 64/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9299 - categorical_crossentropy: 0.9299 - categorical_accuracy: 0.7180 - top_3_accuracy: 0.8889 - val_loss: 1.0553 - val_categorical_crossentropy: 1.0553 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8672\n",
            "Epoch 65/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 0.9105 - categorical_crossentropy: 0.9105 - categorical_accuracy: 0.7271 - top_3_accuracy: 0.8956 - val_loss: 1.0644 - val_categorical_crossentropy: 1.0644 - val_categorical_accuracy: 0.7070 - val_top_3_accuracy: 0.8574\n",
            "Epoch 66/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.9069 - categorical_crossentropy: 0.9069 - categorical_accuracy: 0.7305 - top_3_accuracy: 0.8952 - val_loss: 1.0516 - val_categorical_crossentropy: 1.0516 - val_categorical_accuracy: 0.7109 - val_top_3_accuracy: 0.8691\n",
            "Epoch 67/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.9061 - categorical_crossentropy: 0.9061 - categorical_accuracy: 0.7256 - top_3_accuracy: 0.8948 - val_loss: 1.0580 - val_categorical_crossentropy: 1.0580 - val_categorical_accuracy: 0.7109 - val_top_3_accuracy: 0.8652\n",
            "Epoch 68/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8983 - categorical_crossentropy: 0.8983 - categorical_accuracy: 0.7311 - top_3_accuracy: 0.8967 - val_loss: 1.0492 - val_categorical_crossentropy: 1.0492 - val_categorical_accuracy: 0.7148 - val_top_3_accuracy: 0.8652\n",
            "Epoch 69/100\n",
            "216/218 [============================>.] - ETA: 0s - loss: 0.8958 - categorical_crossentropy: 0.8958 - categorical_accuracy: 0.7328 - top_3_accuracy: 0.8966\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8952 - categorical_crossentropy: 0.8952 - categorical_accuracy: 0.7329 - top_3_accuracy: 0.8966 - val_loss: 1.0632 - val_categorical_crossentropy: 1.0632 - val_categorical_accuracy: 0.7129 - val_top_3_accuracy: 0.8711\n",
            "Epoch 70/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8967 - categorical_crossentropy: 0.8967 - categorical_accuracy: 0.7312 - top_3_accuracy: 0.8980 - val_loss: 1.0514 - val_categorical_crossentropy: 1.0514 - val_categorical_accuracy: 0.7148 - val_top_3_accuracy: 0.8691\n",
            "Epoch 71/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.9047 - categorical_crossentropy: 0.9047 - categorical_accuracy: 0.7266 - top_3_accuracy: 0.8931 - val_loss: 1.0470 - val_categorical_crossentropy: 1.0470 - val_categorical_accuracy: 0.7148 - val_top_3_accuracy: 0.8711\n",
            "Epoch 72/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8844 - categorical_crossentropy: 0.8844 - categorical_accuracy: 0.7340 - top_3_accuracy: 0.8960 - val_loss: 1.0558 - val_categorical_crossentropy: 1.0558 - val_categorical_accuracy: 0.7129 - val_top_3_accuracy: 0.8652\n",
            "Epoch 73/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.9091 - categorical_crossentropy: 0.9091 - categorical_accuracy: 0.7251 - top_3_accuracy: 0.8917 - val_loss: 1.0542 - val_categorical_crossentropy: 1.0542 - val_categorical_accuracy: 0.7090 - val_top_3_accuracy: 0.8672\n",
            "Epoch 74/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8943 - categorical_crossentropy: 0.8943 - categorical_accuracy: 0.7268 - top_3_accuracy: 0.8980 - val_loss: 1.0514 - val_categorical_crossentropy: 1.0514 - val_categorical_accuracy: 0.7090 - val_top_3_accuracy: 0.8672\n",
            "Epoch 75/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8917 - categorical_crossentropy: 0.8917 - categorical_accuracy: 0.7308 - top_3_accuracy: 0.9007 - val_loss: 1.0433 - val_categorical_crossentropy: 1.0433 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8691\n",
            "Epoch 76/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.8937 - categorical_crossentropy: 0.8937 - categorical_accuracy: 0.7311 - top_3_accuracy: 0.8965\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8945 - categorical_crossentropy: 0.8945 - categorical_accuracy: 0.7308 - top_3_accuracy: 0.8960 - val_loss: 1.0517 - val_categorical_crossentropy: 1.0517 - val_categorical_accuracy: 0.7129 - val_top_3_accuracy: 0.8711\n",
            "Epoch 77/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8913 - categorical_crossentropy: 0.8913 - categorical_accuracy: 0.7325 - top_3_accuracy: 0.9007 - val_loss: 1.0501 - val_categorical_crossentropy: 1.0501 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8691\n",
            "Epoch 78/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8899 - categorical_crossentropy: 0.8899 - categorical_accuracy: 0.7329 - top_3_accuracy: 0.8963 - val_loss: 1.0412 - val_categorical_crossentropy: 1.0412 - val_categorical_accuracy: 0.7148 - val_top_3_accuracy: 0.8711\n",
            "Epoch 79/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.8816 - categorical_crossentropy: 0.8816 - categorical_accuracy: 0.7339 - top_3_accuracy: 0.8996 - val_loss: 1.0391 - val_categorical_crossentropy: 1.0391 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8691\n",
            "Epoch 80/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8861 - categorical_crossentropy: 0.8861 - categorical_accuracy: 0.7305 - top_3_accuracy: 0.9007 - val_loss: 1.0485 - val_categorical_crossentropy: 1.0485 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8691\n",
            "Epoch 81/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8991 - categorical_crossentropy: 0.8991 - categorical_accuracy: 0.7274 - top_3_accuracy: 0.8971 - val_loss: 1.0431 - val_categorical_crossentropy: 1.0431 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8691\n",
            "Epoch 82/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8721 - categorical_crossentropy: 0.8721 - categorical_accuracy: 0.7372 - top_3_accuracy: 0.8985 - val_loss: 1.0394 - val_categorical_crossentropy: 1.0394 - val_categorical_accuracy: 0.7207 - val_top_3_accuracy: 0.8730\n",
            "Epoch 83/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.8871 - categorical_crossentropy: 0.8871 - categorical_accuracy: 0.7360 - top_3_accuracy: 0.8966\n",
            "Epoch 00083: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "218/218 [==============================] - 14s 62ms/step - loss: 0.8879 - categorical_crossentropy: 0.8879 - categorical_accuracy: 0.7355 - top_3_accuracy: 0.8965 - val_loss: 1.0381 - val_categorical_crossentropy: 1.0381 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8672\n",
            "Epoch 84/100\n",
            "218/218 [==============================] - 13s 62ms/step - loss: 0.8846 - categorical_crossentropy: 0.8846 - categorical_accuracy: 0.7338 - top_3_accuracy: 0.8999 - val_loss: 1.0369 - val_categorical_crossentropy: 1.0369 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8652\n",
            "Epoch 85/100\n",
            "218/218 [==============================] - 14s 62ms/step - loss: 0.8853 - categorical_crossentropy: 0.8853 - categorical_accuracy: 0.7340 - top_3_accuracy: 0.8985 - val_loss: 1.0406 - val_categorical_crossentropy: 1.0406 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8672\n",
            "Epoch 86/100\n",
            "218/218 [==============================] - 14s 62ms/step - loss: 0.8660 - categorical_crossentropy: 0.8660 - categorical_accuracy: 0.7375 - top_3_accuracy: 0.9017 - val_loss: 1.0364 - val_categorical_crossentropy: 1.0364 - val_categorical_accuracy: 0.7207 - val_top_3_accuracy: 0.8633\n",
            "Epoch 87/100\n",
            "218/218 [==============================] - 13s 62ms/step - loss: 0.8884 - categorical_crossentropy: 0.8884 - categorical_accuracy: 0.7308 - top_3_accuracy: 0.8964 - val_loss: 1.0363 - val_categorical_crossentropy: 1.0363 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8652\n",
            "Epoch 88/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8743 - categorical_crossentropy: 0.8743 - categorical_accuracy: 0.7344 - top_3_accuracy: 0.9018 - val_loss: 1.0348 - val_categorical_crossentropy: 1.0348 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8672\n",
            "Epoch 89/100\n",
            "218/218 [==============================] - 14s 65ms/step - loss: 0.8948 - categorical_crossentropy: 0.8948 - categorical_accuracy: 0.7304 - top_3_accuracy: 0.8960 - val_loss: 1.0360 - val_categorical_crossentropy: 1.0360 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8652\n",
            "Epoch 90/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.8540 - categorical_crossentropy: 0.8540 - categorical_accuracy: 0.7451 - top_3_accuracy: 0.9024\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8537 - categorical_crossentropy: 0.8537 - categorical_accuracy: 0.7451 - top_3_accuracy: 0.9025 - val_loss: 1.0348 - val_categorical_crossentropy: 1.0348 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8652\n",
            "Epoch 91/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8782 - categorical_crossentropy: 0.8782 - categorical_accuracy: 0.7378 - top_3_accuracy: 0.8986 - val_loss: 1.0363 - val_categorical_crossentropy: 1.0363 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8672\n",
            "Epoch 92/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8853 - categorical_crossentropy: 0.8853 - categorical_accuracy: 0.7338 - top_3_accuracy: 0.8993 - val_loss: 1.0391 - val_categorical_crossentropy: 1.0391 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8672\n",
            "Epoch 93/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8717 - categorical_crossentropy: 0.8717 - categorical_accuracy: 0.7364 - top_3_accuracy: 0.9019 - val_loss: 1.0368 - val_categorical_crossentropy: 1.0368 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8652\n",
            "Epoch 94/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8655 - categorical_crossentropy: 0.8655 - categorical_accuracy: 0.7377 - top_3_accuracy: 0.9014 - val_loss: 1.0373 - val_categorical_crossentropy: 1.0373 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8652\n",
            "Epoch 95/100\n",
            "218/218 [==============================] - 14s 64ms/step - loss: 0.8895 - categorical_crossentropy: 0.8895 - categorical_accuracy: 0.7323 - top_3_accuracy: 0.8944 - val_loss: 1.0405 - val_categorical_crossentropy: 1.0405 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8691\n",
            "Epoch 96/100\n",
            "218/218 [==============================] - 14s 66ms/step - loss: 0.8860 - categorical_crossentropy: 0.8860 - categorical_accuracy: 0.7333 - top_3_accuracy: 0.8987 - val_loss: 1.0417 - val_categorical_crossentropy: 1.0417 - val_categorical_accuracy: 0.7129 - val_top_3_accuracy: 0.8672\n",
            "Epoch 97/100\n",
            "217/218 [============================>.] - ETA: 0s - loss: 0.8694 - categorical_crossentropy: 0.8694 - categorical_accuracy: 0.7372 - top_3_accuracy: 0.9012\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8686 - categorical_crossentropy: 0.8686 - categorical_accuracy: 0.7373 - top_3_accuracy: 0.9013 - val_loss: 1.0388 - val_categorical_crossentropy: 1.0388 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8652\n",
            "Epoch 98/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8688 - categorical_crossentropy: 0.8688 - categorical_accuracy: 0.7382 - top_3_accuracy: 0.9012 - val_loss: 1.0414 - val_categorical_crossentropy: 1.0414 - val_categorical_accuracy: 0.7188 - val_top_3_accuracy: 0.8672\n",
            "Epoch 99/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8737 - categorical_crossentropy: 0.8737 - categorical_accuracy: 0.7377 - top_3_accuracy: 0.9021 - val_loss: 1.0427 - val_categorical_crossentropy: 1.0427 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8652\n",
            "Epoch 100/100\n",
            "218/218 [==============================] - 14s 63ms/step - loss: 0.8805 - categorical_crossentropy: 0.8805 - categorical_accuracy: 0.7343 - top_3_accuracy: 0.8971 - val_loss: 1.0417 - val_categorical_crossentropy: 1.0417 - val_categorical_accuracy: 0.7168 - val_top_3_accuracy: 0.8652\n",
            "Test loss: 1.0416748523712158\n",
            "Test accuracy: 1.0416748523712158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hdkSZI2NrRAQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_cnn = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jW2SJXAu0Um",
        "colab_type": "code",
        "outputId": "5b86c794-0b86-4e22-952d-84d231d870f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "test = []\n",
        "for c, img in enumerate(X_test):\n",
        "    img = img.reshape(100, 100)\n",
        "    test.append(remove_noise(img, 100).flatten())\n",
        "    \n",
        "for c, img in enumerate(test):\n",
        "    test[c] = np.array(img)\n",
        "test = np.array(test)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:118: UserWarning: Possible sign loss when converting negative image of type float64 to positive image of type bool.\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to bool\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hW-CTFsrLpaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_cropped = crop_image(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtCICMzUKlQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_x = test_cropped\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    test_x = test_x.reshape(test_x.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    test_x = test_x.reshape(test_x.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "test_x = test_x.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfFEea4Y1HVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58837612-35a5-40b5-dd60-fd86151f0de1"
      },
      "cell_type": "code",
      "source": [
        "prediction_cnn = model_cnn.predict_classes(test_x, batch_size=1, verbose=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 21s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WZe3X4gs1PEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "293ebe41-cc25-4ea5-8141-1ffde288578c"
      },
      "cell_type": "code",
      "source": [
        "print((prediction_cnn))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16 14 26 ... 27 16  9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s5zJH74T1HaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c8dba30-04fa-44e0-a5c7-299ccdc670f8"
      },
      "cell_type": "code",
      "source": [
        "prediction_mn = model_mn.predict(test_x, batch_size=1, verbose=1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 59s 6ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N6jz3gFn2WX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cd5ef31-d606-410f-d5b8-526200bf67f8"
      },
      "cell_type": "code",
      "source": [
        "print(type(prediction_mn))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3XOU-rXD2KbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22701fc1-d88d-49d3-a363-b942fc709834"
      },
      "cell_type": "code",
      "source": [
        "prediction_mn = np.argmax(prediction_mn,axis=1)\n",
        "print(prediction_mn)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16 14 26 ... 21 16  9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-sl6hezJ2kjf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "temp = prediction_mn - prediction_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ireSZy9H2kor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d98cf8c-b697-4800-e418-98c8b396e6a0"
      },
      "cell_type": "code",
      "source": [
        "prediction_cnn = model_cnn.predict(test_x, batch_size=1, verbose=1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 21s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-07X1XH94bAi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sum_ = prediction_cnn + prediction_mn\n",
        "sum_ = np.argmax(sum_,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBDQ2Qto4bJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oHZ9Iqua4bHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_sum = []\n",
        "for index, image in enumerate(sum_):\n",
        "  result_sum.append(cat_map[image])\n",
        "  \n",
        "result_sum = pd.DataFrame(result_sum)\n",
        "result_sum = result_sum.reset_index(drop=False)\n",
        "result_sum.columns=['Id', 'Category']\n",
        "result_sum.to_csv('result_sum.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gMQ7BlsMl59",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "result_cnn, result_mn = [], []\n",
        "labels_df = pd.read_csv(path_to_labels)\n",
        "labels_df.Category = pd.Categorical(labels_df.Category)\n",
        "cat_map = pd.Categorical(labels_df.Category)\n",
        "cat_map = cat_map.categories\n",
        "\n",
        "for index, image in enumerate(prediction_cnn):\n",
        "  result_cnn.append(cat_map[image])\n",
        "\n",
        "result_cnn = pd.DataFrame(result_cnn)\n",
        "result_cnn = result_cnn.reset_index(drop=False)\n",
        "result_cnn.columns=['Id', 'Category']\n",
        "result_cnn.to_csv('result_cnn.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "for index, image in enumerate(prediction_mn):\n",
        "  result_mn.append(cat_map[image])\n",
        "  \n",
        "result_mn = pd.DataFrame(result_mn)\n",
        "result_mn = result_mn.reset_index(drop=False)\n",
        "result_mn.columns=['Id', 'Category']\n",
        "result_mn.to_csv('result_mn.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ONxnOMQXVy7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f62dfec-3ee1-4b8a-e892-e375aad22a9d"
      },
      "cell_type": "code",
      "source": [
        "print(type(result_cnn))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0x801SC034fC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        },
        "outputId": "03b42870-fbb7-4592-dc60-b105a980c8a6"
      },
      "cell_type": "code",
      "source": [
        "print(result_sum)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Id      Category\n",
            "0        0     pineapple\n",
            "1        1       penguin\n",
            "2        2          sink\n",
            "3        3         empty\n",
            "4        4         empty\n",
            "5        5         empty\n",
            "6        6      scorpion\n",
            "7        7      scorpion\n",
            "8        8     pineapple\n",
            "9        9      scorpion\n",
            "10      10        pencil\n",
            "11      11     pineapple\n",
            "12      12          sink\n",
            "13      13         skull\n",
            "14      14         panda\n",
            "15      15        pencil\n",
            "16      16         rifle\n",
            "17      17     pineapple\n",
            "18      18  rollerskates\n",
            "19      19         rifle\n",
            "20      20      sailboat\n",
            "21      21          pool\n",
            "22      22          nail\n",
            "23      23      scorpion\n",
            "24      24         panda\n",
            "25      25         mouth\n",
            "26      26      scorpion\n",
            "27      27      scorpion\n",
            "28      28        rabbit\n",
            "29      29        shovel\n",
            "...    ...           ...\n",
            "9970  9970    skateboard\n",
            "9971  9971        parrot\n",
            "9972  9972      squiggle\n",
            "9973  9973          pool\n",
            "9974  9974         mouth\n",
            "9975  9975      scorpion\n",
            "9976  9976        rabbit\n",
            "9977  9977         panda\n",
            "9978  9978          nail\n",
            "9979  9979      scorpion\n",
            "9980  9980     moustache\n",
            "9981  9981      scorpion\n",
            "9982  9982           mug\n",
            "9983  9983       penguin\n",
            "9984  9984     pineapple\n",
            "9985  9985        rabbit\n",
            "9986  9986     pineapple\n",
            "9987  9987       penguin\n",
            "9988  9988         empty\n",
            "9989  9989           mug\n",
            "9990  9990        shovel\n",
            "9991  9991          sink\n",
            "9992  9992         mouth\n",
            "9993  9993        parrot\n",
            "9994  9994        rabbit\n",
            "9995  9995         rifle\n",
            "9996  9996        pencil\n",
            "9997  9997    skateboard\n",
            "9998  9998     pineapple\n",
            "9999  9999         panda\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i5lRNhqfNTha",
        "colab_type": "code",
        "outputId": "ea35776d-f0e7-4a7f-91fd-ff1ce0cfb2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "count = 0 \n",
        "for c, (a, b) in enumerate(zip(result_cnn.values, result_mn.values)):\n",
        "  s = result_sum.values[c][1]\n",
        "  if a[1] != s and b[1] != s:\n",
        "#     print(s)\n",
        "#     print (c, a[1], b[1])\n",
        "    count +=1 \n",
        "print(count)\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jutaTXV37uqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tno1Yg0G5kHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1337
        },
        "outputId": "4f5f255e-86f1-44c1-e201-c7e386092379"
      },
      "cell_type": "code",
      "source": [
        "plot_images([test_x[1476].reshape(32,32)])\n",
        "plot_images([test_x[1526].reshape(32,32)])\n",
        "plot_images([test_x[1589].reshape(32,32)])\n",
        "plot_images([test_x[1644].reshape(32,32)])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABQtJREFUeJzt3UtuGzEQQMEwmPtfmbmB9eCMyPlU\nrQ2I1uKhF01xzDnnHwB+9Hf3AQDuQCwBArEECMQSIBBLgEAsAYJjxYeMMVZ8DMB/+WmT0mQJEIgl\nQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCB\nWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJ\nEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAcOw+AM8w59x9hF8bY+w+AjdgsgQIxBIgEEuA\nQCwBArEECMQSIBBLgEAsAQJL6bxeXai3vP5uJkuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiCwlL7I\nmb8kvno5+s6/gg5nMVkCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEbvCcwA2Xc5UbSr5zVjNZ\nAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgaX0GyoL2aufnqiuei74xGQJEIglQCCWAIFYAgRiCRCI\nJUAglgCBWAIEltI/ePovcl/x/7vimcBkCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJELjBw+t5\n6oLCZAkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgSW0llq9ZMRFs45i8kSIBBLgEAsAQKxBAjEEiAQ\nS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4DAsxLckuciWM1kCRCIJUAg\nlgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgC\nBGIJEIglQCCWAIFYAgRiCRCIJUAglgDBsfsAVzfG+Pg3c84FJwF2MlkCBGIJEIglQCCWAIFYAgRi\nCRCIJUAglgCBpfSHsigP5zJZAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBG7wnMDTE/B8JkuA\nQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKx\nBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECI7dB+Bdxhi7jwC/YrIECMQSIBBLgEAsAQKxBAjE\nEiAQS4BALAECS+knmHPuPgLwZSZLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQS\nIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BA\nLAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEE\nCMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIjt0H4BnGGLuPAF9lsgQIxBIgEEuAQCwBArEECMQS\nIBBLgEAsAQJL6ScoC9lzzgUn+Q4L52CyBEjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECNzgWeSq\nt2DKzaLVt4+u+l3xbiZLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgGPPO7x0ALGKyBAjEEiAQS4BA\nLAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEE\nCMQSIBBLgOAf+kVDrFEXVyIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f850f01b518>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABRFJREFUeJzt3bGO2zAUAEEx8P//MtMF11jYgAol\nxzP1wdI1i1c8imPOOQ8ATv26+wUAPoFYAgRiCRCIJUAglgCBWAIErx0PGWPseAzAkrNNSpMlQCCW\nAIFYAgRiCRCIJUAglgCBWAIEYgkQbFlKB7jb6qd7TZYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAg\nlgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgSulQA+2up1EZXJEiAQS4BALAECsQQIxBIg\nEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAwLUSwEcbY6S/W71+wmQJ\nEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEltKBr1CW188W102WAIFYAgRiCRCIJUAglgCBWAIEYgkQ\niCVAYCkdeKzVr5v/VL+o/o7JEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIHCCB7jFladzdjzP\nZAkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgSW0oFL7V4238VkCRCIJUAglgCBWAIEYgkQiCVAIJYA\ngVgCBJbSgex/XTgvTJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgC\nBGIJEIglQCCWAIFYAgSulQCO43jmlRFjjMt+a/X/M1kCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCB\npXTgFlcunF/1vLPFdZMlQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCW\nAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgSvu18A4J05592v8IfJ\nEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuA\nwLUSwGONMS77rdUrKkyWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgRM8wFcop4HOTvmYLAEC\nsQQIxBIgEEuAQCwBArEECMQSIBBLgMBSOnAcx/rS9t8qv3XltRKrTJYAgVgCBGIJEIglQCCWAIFY\nAgRiCRCIJUBgKR14rCctrpssAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBAid4gGz31RNFfd7q\nSR+TJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEFhKBy5Vl7+furz+jskSIBBLgEAsAQKxBAjEEiAQ\nS4BALAECsQQILKUDt3jiV9fPmCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECJ3iAx6pXVBSu\nlQDYQCwBArEECMQSIBBLgEAsAQKxBAjEEiCwlA58hdVrLEyWAIFYAgRiCRCIJUAglgCBWAIEYgkQ\niCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCW\nAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIE\nYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQvHY8ZM654zEA/4zJEiAQ\nS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwB\nArEECMQSIBBLgEAsAYLfXC5Xoe7NghYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f850f01b0f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABNBJREFUeJzt3TFuwzAQAMEw0P+/TLdxYy1ghaaS\nmdqwVC2uOIpjzjm/AHjp+9MvAHAHYgkQiCVAIJYAgVgCBGIJEBwrHjLGWPEYgLe82qQ0WQIEYgkQ\niCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCW\nAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIE\nYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIgl\nQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCB\nWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJ\nEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAg\nlgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgDB8ekX4Nmc8/Q3Y4wFbwL8\nZLIECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQInOC5QDl1s+vznAaCxmQJEIglQCCWAIFYAgRi\nCRCIJUAglgCBWAIEltL/OddYQGOyBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBAkvpJ1Z/BR3Yk8kS\nIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgcIKHU66eAJMlQCKWAIFYAgRiCRCIJUAglgCBWAIE\nYgkQWErnEvX6Dcvr3JXJEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIHCC50Q5cVJPr6xUT8rs\n+O6wI5MlQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQWEq/wI6L61de83Dlu5f/cvUEOzJZAgRiCRCI\nJUAglgCBWAIEYgkQiCVAIJYAgaX0RXb9crkvpUNjsgQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKx\nBAic4NnMjldUACZLgEQsAQKxBAjEEiAQS4BALAECsQQIxBIgsJTOqXolBvxlJkuAQCwBArEECMQS\nIBBLgEAsAQKxBAjEEiCwlH5DlsRhPZMlQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgC\nBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCI\nJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYA\ngVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRi\nCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAMGx4iFzzhWPAfg1JkuAQCwBArEECMQS\nIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BA\nLAECsQQIHot1PKM0v+g3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f850efc6128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABIBJREFUeJzt28EJwzAQAMEouP+WLx3ECwHLDjMN\n6F7LPU5rZuYFwFfv3QMAPIFYAgRiCRCIJUAglgCBWAIExxWPrLWueAbgJ98uKW2WAIFYAgRiCRCI\nJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYA\ngVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRi\nCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVA\nIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFY\nAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQ\niCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCW\nAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIE\nYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIgl\nQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCB\nWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJ\nEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAg\nlgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgC\nBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCI\nJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYA\ngVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRi\nCRCIJUAglgCBWAIEx+4B7m5mdo+w3Vpr9wiwnc0SIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIHKXz\nSHf9LOCA/3/ZLAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQI/eLjUXX/ewBmbJUAglgCBWAIE\nYgkQiCVAIJYAgVgCBGIJEKxxJQxwymYJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYA\ngVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAwQflxxaQb6keJQAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f850ef8b710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5z-3cDz86CU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}